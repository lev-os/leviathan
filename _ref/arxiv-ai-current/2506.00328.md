## BASIL: Best-Action Symbolic Interpretable Learning for Evolving Compact RL Policies

Kourosh Shahnazari [1*] _**[†]**_, Seyed Moein Ayyoubzadeh [1] _**[†]**_ and
Mohammadali Keshtparvar [2]

1* Sharif University of Technology, Tehran, Iran.
2* Amirkabir University of Technology, Tehran, Iran.

*Corresponding author(s). E-mail(s): kourosh@null.net;
Contributing authors: smoein.ayyoubzadeh16@sharif.edu;

mohammad.kp2000@aut.ac.ir;

_**†**_ These authors contributed equally to this work.

**Abstract**

The quest for interpretable reinforcement learning is a grand challenge for the
deployment of autonomous decision-making systems in safety-critical applications. Modern deep reinforcement learning approaches, while powerful, tend to
produce opaque policies that compromise verification, reduce transparency, and
impede human oversight. To tackle this, we introduce BASIL ( _Best-Action Sym-_
_bolic Interpretable Learning_ ), a systematic approach for generating symbolic,
rule-based policies via online evolutionary search with quality-diversity (QD)
optimization. BASIL represents policies as ordered lists of symbolic predicates
over state variables, ensuring full interpretability and tractable policy complexity. By using a QD archive, the methodology in the proposed study encourages
behavioral and structural diversity between top-performing solutions, while a
complexity-aware fitness encourages the synthesis of compact representations.
The evolutionary system supports the use of exact constraints for rule count
and system adaptability for balancing transparency with expressiveness. Empirical comparisons with three benchmark tasks— `CartPole-v1`, `MountainCar-v0`,
and `Acrobot-v1` —show that BASIL consistently synthesizes interpretable controllers with compact representations comparable to deep reinforcement learning
baselines. Herein, this article introduces a new interpretable policy synthesis
method that combines symbolic expressiveness, evolutionary diversity, and online
learning through a unifying framework.

**Keywords:** Interpretable RL, Symbolic Policy Learning, Genetic Algorithms,
Rule-Based Agents, Explainable AI, Policy Simplification, Real-Time Policy Evolution

1

### **1 Introduction**

Reinforcement Learning has demonstrated itself as a core method for solving complex decision-making and control problems in a range of domains, such as robotic
control, autonomous vehicles, financial trading, and healthcare systems [1, 2]. Latest
advancements in Deep Reinforcement Learning (DRL) techniques, based on strategies such as Deep Q-Networks (DQN)[3], Proximal Policy Optimization (PPO)[4], and
Soft Actor-Critic (SAC)[5], have proved capable of achieving state-of-the-art performance for tasks with high-dimensional state and action spaces. Nonetheless, in spite of
their empirical success, DRL algorithms tend to produce non-transparent, opaque policies, with important interpretability and reliability drawbacks in deployment domains
wherein safety is a top concern. Transparency issues with such policies weaken the
basis for building confidence and accountability, as well as the potential for human
monitoring, in domains wherein transparent decision-making procedures are crucially
important[6, 7].
Unlike traditional Deep Reinforcement Learning (DRL) approaches, Evolutionary Reinforcement Learning (ERL) techniques, including Neuroevolution [8], Genetic
Programming (GP)[9], and Learning Classifier Systems (LCS)[10], offer promising
directions for creating understandable and interpretable policies. While these evolutionary methods naturally produce a structured form of representation that can
be easily understood by humans, they often suffer from trade-offs between maximizing performance and maintaining simplicity in the policy structure. Consequently,
solutions based on ERL tend to be subject either to overcomplexity, undermining
interpretability, or oversimplification, in turn, reducing the performance level [11].
To robustly address this critical gap in existing approaches, we introduce BASIL
(Best-Action Symbolic Interpretable Learning), a novel, state-of-the-art framework
based on Genetic Rule-based Reinforcement Learning (RL). Our proposed algorithm
is the first attempt at combining Genetic Algorithms (GA)[12] and Quality-Diversity
(QD) archives[13] in a systematic way such that it generates symbolically interpretable,
rule-based policies defined in terms of logical predicates over state variables. The
major contribution of our approach lies in its explicit focus on the evolution of concise
yet effective sets of rules, thereby attaining high interpretability with little or no
reduction in the performance metrics. Additionally, the use of a QD-guided archive
helps maintain a highly diverse set of top-performing policies in different behavioral
and structural aspects. Not only does this enhance interpretability, but it also helps
in exploring the policy space effectively.
Specifically, the genetic operators used in our approach—i.e., mutation and
crossover—are carefully designed to maintain a simple policy framework, while a
strategically introduced complexity penalty suitably guides the evolutionary process
towards more compact rule sets [14]. Another significant advantage of our method
is its ability to predetermine the number of rules required, and hence provide users
with exact control over policy complexity. This feature is especially useful in practical
applications, as it enables domain experts to skillfully express the trade-off between
interpretability and decision accuracy. Furthermore, the capability to determine policy complexity a priori improves both scalability and tractability when extended to

2

real-world problems, and hence considerably expands the scope of our algorithm’s
applicability.
Additionally, the use of an advanced QD archive greatly enhances the evolutionary exploration process. By taking advantage of features like policy length and mean
thresholds of predicates, our archive systematically guarantees complete coverage of
behavior [15]. This new combination encourages the evolution of innovative, yet consistently high-performing strategies, thus enabling stakeholders to select from a range
of policies crafted for a variety of operational environments or interpretability requirements. Therefore, BASIL not only successfully addresses the interpretability aspect
but also greatly augments exploration in the solution space, generating a rich and
diverse range of behaviors which are commonly ignored by traditional evolutionary
approaches [13].
We show the effectiveness and interpretability of BASIL through systematic experimental validation on standard benchmark reinforcement learning domains, namely
CartPole, MountainCar, and Acrobot. Our experiments suggest our methodology
generates a wide range of interpretable policies with state-of-the-art performance in
comparison with top deep reinforcement learning techniques, while vastly outperforming traditional ERL-based approaches in terms of interpretability as well as policy
diversity [16]. Our experimental methodology consists of exhaustive ablation evaluations, effectively demonstrating the effectiveness and useful contribution of every
component in our proposed methodology. Further analysis discovers our quality diversity archive helps in the selection of a wide range of insightful policy variations, leading
towards an advanced understanding of the underlying decision-making dynamics as
well as emphasizing the real-world and theoretical advantages of our approach.
This paper introduces BASIL, a novel reinforcement learning framework for the
construction of concise, symbolic policies through the evolution of genetic rules under
the assistance of a Quality-Diversity archive. BASIL builds on the state-of-the-art in
interpretable reinforcement learning by combining the online evolution of policies, a
logical rule-based system, and diversity-focused optimization in a single framework.
With experiments spanning a range of control benchmarks, we show that BASIL
generates highly interpretable controllers with performance matching deep reinforcement learning and prior evolutionary state-of-the-arts in most cases. Our analysis of
the diversity of policies in the Quality-Diversity archive highlights its essential role
in driving the exploration of structurally diverse and semantically expressive policies and provides new insights into the interplay between exploration, expressiveness,
and the interpretability of symbolic policy learning. Accordingly, BASIL constitutes a
major innovation in the area of interpretable RL, providing unparalleled transparency,
flexibility, and range of solutions for complex decision-making tasks.
### **2 Related Work**

The pursuit of interpretable reinforcement learning has caught significant attention,
leading to a great variety of approaches that attempt to balance performance and
interpretability. Our approach stands out in that it evolves compact, symbolic rulebased policies through online environment interactions, with a particular focus on

3

both interpretability and diversity. In what follows, we place our work in the context
of existing research.

**2.1 Evolutionary Approaches to Interpretable RL**

**Decision Tree-Based Methods.** Custode and Iacca introduced an evolutionary
algorithm that learns interpretable decision trees for RL tasks. Their method combines
evolutionary strategies with Q-learning to associate actions with tree leaves, focusing
on discrete action spaces and employing a two-level optimization scheme to decompose
the state space effectively [14]. While their approach emphasizes interpretability, it
does not incorporate mechanisms for preserving behavioral diversity or exploring a
wide range of policy structures.
**Genetic Programming Techniques.** Hein and coworkers proposed the Genetic
Programming for Reinforcement Learning (GPRL) framework, which infers algebraic
policy equations from pre-existing trajectory data with model-based batch reinforcement learning methods [11]. Although GPRL policies are interpretable, the framework
relies on offline trajectories and does not have methods for online adaptation or
diversity preservation.
**Co-evolutionary Strategies.** Later, Custode and Iacca extended their approach
to continuous action spaces with a cooperative co-evolutionary algorithm utilizing
binary decision trees evolved with the aid of grammatical evolution [17]. Though such
an approach easily addresses issues with continuous control, it does not explicitly focus
on maintaining a variety of effective policies.

**2.2 Post-hoc Interpretability and Program Synthesis**

**Evolutionary Feature Synthesis.** Zhang et al. proposed a method that extracts
interpretable policies from pre-trained deep neural networks with the help of evolutionary feature synthesis (EFS). They proposed a set of regressors individually designed
in order to mimic the behavior of a neural policy, followed by a simplification step
in order to improve interpretability [18]. This, however, is a post-hoc approach based
on having access to a pre-trained model, thus limiting its applicability in situations
involving online learning.
**Programmatically Interpretable RL.** Verma et al. proposed the idea of Programmatically Interpretable Reinforcement Learning (PIRL), which represents policies
as programs in a high-level language. Within this approach, a neural policy acts as
an oracle for generating interpretable programs with similar behavior as the neural
policy [19]. Although the policies learned by PIRL are interpretable and verifiable,
the method is retrospective and does not learn policies actively through environment
interactions.

**2.3 Quality-Diversity Optimization in Interpretable RL**

**Quality-Diversity in Decision Trees.** Ferigo et al. applied Quality-Diversity optimization methods, such as MAP-Elites, to evolve decision trees for interpretable
reinforcement learning (RL) [16]. By maintaining a diverse set of high-quality policies,
the method enhances exploration and robustness in the learning of policies. Even so,

4

their method mainly focuses on decision trees and does not employ symbolic rule-based
representations.
**Neural Symbolic Reinforcement Learning.** Ma et al. introduced a Neural
Symbolic Reinforcement Learning framework that integrates symbolic logic into deep
RL. This approach enables end-to-end learning with prior symbolic knowledge, enhancing interpretability by extracting logical rules learned by the reasoning module [20].
While promising, this method relies on neural networks and does not evolve policies
through direct environment interactions.

**2.4 Other Notable Approaches**

**Behavior Tree Extraction.** Recent studies have explored deriving interpretable
behavior trees from RL policies. These methods focus on translating learned policies
into behavior trees, facilitating human understanding and verification. Nevertheless, they often lack mechanisms for ensuring diversity or simplicity in the resulting

structures.

**Fuzzy Controllers and Neuro-Fuzzy Systems.** Hein et al. studied the use
of fuzzy controllers for interpretable reinforcement learning, leveraging genetic programming and particle swarm optimization to evolve fuzzy rule sets [21]. While these
methods result in interpretable policies, they may need large rule sets, potentially
compromising simplicity.
**Symbolic Regression for Value Functions.** Babuˇska investigated the potential of genetic programming and symbolic regression to obtain symbolic expressions
of value functions in RL [22]. This work concentrates on value function approximation rather than direct policy derivation, yielding insights into interpretable function
approximation techniques.
**Program Synthesis for Policy Learning.** Liventsev et al. proposed BF++, a
language designed for general-purpose program synthesis in RL contexts [23]. Their
approach emphasizes the design of interpretable programs encoding policies, though
it does not clearly account for policy diversity or simplicity.
**Neuro-Fuzzy System Distillation.** Gevaert et al. presented a method for
distilling deep RL models into interpretable neuro-fuzzy systems [24]. This framework attempts to integrate the performance of deep reinforcement learning into
the interpretability of fuzzy systems, though it may not guarantee compact policy
representations.
**Symbolic Policy Learning.** Landajuela et al. introduced Deep Symbolic Policy
(DSP), a method that investigates for tractable mathematical expressions representing policies [25]. DSP emphasizes the discovery of symbolic policies with strong
generalization capabilities, aligning with the goals of interpretability and simplicity.
**Explainable RL via Symbolic Logic.** Recent studies have explored integrating
symbolic logic into RL to enhance interpretability. These approaches aim to produce
explanations where key concepts and their relationships are described via intuitive
symbols and rules, enabling human understanding of agent behaviors.

5

**2.5 Distinctiveness of Our Approach**

Our method diverges from the aforementioned works in several key aspects:

- **Online Evolution:** Unlike approaches based on static datasets or traditional,
offline processes, our approach seeks policies through direct interaction with the
environment, thus allowing for real-time adaptability and responsiveness in response
to changing conditions.

- **Rule-Based Representation:** We use ordered lists of Boolean rules, with every
rule acting as a conjunction of predicates based on given thresholds. This arrangement provides a highly interpretable alternative relative to obscure neural policies
and often beats the transparency provided by symbolic algebraic representations or
decision trees.

- **Complexity Control via Penalty Term:** A dedicated complexity penalty in
the fitness function explicitly biases the search toward simpler rule sets. This
allows practitioners to control the trade-off between performance and interpretability—enabling the generation of arbitrarily compact policies depending on task
requirements or domain constraints.

- **Quality-Diversity Mechanism:** The creation of a Quality-Diversity (QD) archive
allows for the maintenance of a set of high-quality strategies with structural
and behavioral diversity. The mechanism stimulates broad exploration, improves
resilience, and leads to a better understanding of the landscape of solutions.

- **Simplicity and Efficiency:** The model consistently derives policies with their
characteristic simplicity and effectiveness, often based on a small number of rules
and predicates. Furthermore, it operates with exceptional computational efficiency,
taking only seconds to converge on standard control tasks.

While existing studies have attempted to investigate interpretability from various
angles, BASIL uniquely unifies online symbolic policy evolution, explicit complexity
management, and QD diversity maintenance. This unification forms a consistent and
highly effective framework for interpretable decision-making agents.
### **3 The BASIL Framework: Methodology and** **Architecture**

**3.1 Policy Representation**

In this section, we describe the operational details of BASIL, our symbolic rule-based
framework, which forms the foundation of our Genetic Rule-based Reinforcement
Learning (RL) framework. One major motivation for this choice of representation is
the built-in interpretability and straightforwardness it enforces, allowing easy understanding by domain experts as well as by practitioners. This essentially bridges the
gap between sophisticated decision-making capabilities and their implementation in a
form open to understanding by humans.
Within a structured context, the expression of policy as a set of condition-action
rules is deliberate, for the benefit of increased clarity and understandability for human
observers. Each rule is specified as a logical combination of predicates relative to the

6

state variables. Rules are sequentially checked from top to bottom upon the arrival of
the environmental state, and the action associated with the first rule whose predicates
are all satisfied will be performed. Such systematic checking provides transparency
and determinacy in the decision-making for each state encountered.
Mathematically, a policy can be expressed as follows:

_π_ = [ _R_ 1 _, R_ 2 _, . . ., R_ _k_ _, R_ _fallback_ ] _,_ (1)

where represents the rule, and is the total number of explicitly defined rules. Each
rule is a tuple consisting of a set of predicates and a corresponding action:

_R_ _i_ = (predicates _i_ _, a_ _i_ ) _._ (2)

Each predicate within the rule is a simple logical statement involving comparisons
between specific state dimensions and predetermined thresholds, formulated as:

_s_ [ _d_ ]; _op_ ; _threshold,_ (3)

where:

- represents the value of the state vector at the dimension,

- denotes a logical operator, specifically either less-than (¡) or greater-than (¿),

- is a scalar value chosen from a domain-relevant, predefined set of threshold values.

Thus, the complete set of predicates for the rule is represented by a conjunction
(logical AND) of multiple such predicates:

predicates _i_ = ( _s_ [ _d_ 1 ]; _op_ 1 ; _th_ 1 ) _∧_ ( _s_ [ _d_ 2 ]; _op_ 2 ; _th_ 2 ) _∧· · · ∧_ ( _s_ [ _d_ _m_ ]; _op_ _m_ ; _th_ _m_ ) _,_ (4)

where denotes the total number of predicates within rule . Notably, the number of
predicates can vary from rule to rule, providing additional flexibility and adaptability
in policy complexity and interpretability.
Hence, the action for every rule is chosen from a small, predefined set of possible actions defined in the structure of the environment. The last piece of the policy
representation, the fallback rule, supports resilience by specifying a default action to
be taken in the event that none of the conditions of the relevant rules are satisfied.
Typically, the fallback action defaults to a conservative or safe alternative (typically
specified as action 0), thus ensuring the policy remains unambiguously defined and
executable in every environmental context.
One of the main features of our approach to representing policies is the built-in
ability to determine and limit the overall number of rules. By specifying the desired
number of rules explicitly, users and domain experts are given precise control over the
level of complexity in the policy. This feature enables fine-tuning based on practical
factors for example computational resources, interpretability requirements, or specific
demands of the application domain. Our methodology, therefore, not only supports
the discovery of such policies but also ensures they meet the interpretability and
operational requirements relevant to their deployment context.

7

Finally, the structured, symbolic rule-based policy representation provides policies
with understandable and interpretable directives. The deliberate and systematic design
of condition-action rules significantly enhances transparency and facilitates human
validation and verification, making it especially suitable for use in safety-related and
high-consequence decision-making applications.

**3.2 Genetic Operators**

To effectively evolve symbolic rule-based policies, we employ a set of carefully designed
genetic operators in our Genetic Rule-based Reinforcement Learning approach. These
include _initialization_, _mutation_, and _crossover_ operators, all of which have been carefully designed to strike a balance between policy interpretability, maintain structural
simplicity, and promote exhaustive exploration of the policy space.

_**Initialization.**_

The evolution cycle starts with an initialization step, wherein an initial population of
policies is randomly generated. Each policy consists of a fixed or random number of
rules, with every rule containing logical predicates over the state variables, paired with
a corresponding discrete action. By this methodology, an initial diversity is ensured,
hence creating a complete and representative starting point for further exploration.
Each rule _R_ _i_ is of the form:




_,_ (5)



_R_ _i_ =







_m_ _i_
� ( _s_ [ _d_ _j_ ] _op_ _j_ _th_ _j_ ) _, a_ _i_

_j_ =1


where _m_ _i_ is the number of predicates, _s_ [ _d_ _j_ ] denotes the _j_ [th] state dimension, _op_ _j_ _∈{<_
_, >}_ is the operator, _th_ _j_ is the threshold, and _a_ _i_ is the action assigned to the rule.

_**Mutation.**_

The mutation operator allows the introduction of intentional changes into existing
policies, thus allowing the algorithm to explore new areas in the search space. We
implement several types of mutation, which are used probabilistically:

- **Predicate Mutation:** Randomly selects a predicate in a rule and mutates its
dimension, operator, or threshold.

- **Action Mutation:** Alters the action of a randomly chosen rule.

- **Rule Addition:** Admits a newly established rule into the policy, as long as the
maximum number of acceptable rules is not exceeded.

- **Rule Removal:** Eliminates a randomly selected rule, provided the policy has more
than one rule.

These mutations preserve structural interpretability while enabling diverse policy
exploration.

8

_**Crossover.**_

Crossover combines two parent policies to produce an offspring by exchanging rule
segments. Given two parent policies _π_ 1 and _π_ 2, and a crossover point _c_, the child policy
is formed by:

_π_ child = [ _R_ 1 [(] _[π]_ [1] [)] _, R_ 2 [(] _[π]_ [1] [)] _, . . ., R_ _c_ [(] _[π]_ [1] [)] _, R_ _c_ [(] _[π]_ +1 [2] [)] _[, . . ., R]_ _k_ [(] _[π]_ [2] [)] ] _,_ (6)

where _R_ _i_ [(] _[π]_ _[j]_ [)] denotes the _i_ [th] rule obtained from policy _π_ _j_ . To maintain interpretability, we enforce a cap on the total number of rules.

_**Complexity-Aware Design.**_

The evolutionary process we have proposed includes a complexity penalty in the fitness
function. The penalty is used to prefer policies with fewer predicates in their structure,
especially when their performance is comparable to that of more complex counterparts.
This regularization mechanism guarantees that the search approach is guided toward
policies that are not only performant but also compact and interpretable.

In summary, our genetic operators are designed to encourage transparency and
make it easier for users to control policy complexity. By their systematic design, they
ensure that policies produced are human-understandable while iteratively improving
performance in successive generations.

**3.3 Fitness Evaluation**

The fitness evaluation mechanism plays a pivotal role in our evolutionary framework
by guiding the selection process toward effective, efficient, and interpretable policies.
To pursue this intention, we design a fitness function that scrupulously incorporates
raw performance with structural simplicity, thereby encouraging the evolution of rule
sets with the twin virtues of robustness and interpretability.

_**Performance Term.**_

Let a given policy _π_ be executed over _E_ independent episodes in the target environment. For each episode _e ∈{_ 1 _,_ 2 _, . . ., E}_, the cumulative return is defined

as:


_R_ _π_ [(] _[e]_ [)] =


_T_ _e_
� _r_ _t_ [(] _[e]_ [)] _[,]_ (7)

_t_ =0


where _r_ _t_ [(] _[e]_ [)] is the reward at time step _t_, and _T_ _e_ is the terminal time step of episode _e_ .
The mean performance across episodes is then computed as:


Performance( _π_ ) = [1]

_E_


_E_
� _R_ _π_ [(] _[e]_ [)] _[.]_ (8)

_e_ =1


_**Complexity Term.**_

To ensure the interpretability of the policy configuration, we impose a structural
penalty based on the overall complexity of the policy. Assume a policy with _k_ rules,

9

represented as _π_ = [ _R_ 1 _, R_ 2 _, . . ., R_ _k_ ]. Its associated complexity is defined as the total
number of predicates across all rules:


Complexity( _π_ ) =


_k_
� _m_ _i_ _,_ (9)

_i_ =1


where _m_ _i_ denotes the number of predicates in rule _R_ _i_ .

_**Final Fitness.**_

The fitness of a policy is defined as the mean performance minus a complexity-based
penalty:
Fitness( _π_ ) = Performance( _π_ ) _−_ _λ ·_ Complexity( _π_ ) _,_ (10)
where _λ ∈_ R _≥_ 0 is a penalty coefficient that controls the trade-off between maximizing
cumulative return and minimizing logical complexity.

_**Interpretability Consideration.**_

This complexity-aware fitness function constitutes a key improvement of our framework, in that it ensures that the evolutionary process is guided not merely by reward
maximization but also by the creation of compact and interpretable rule sets. By carefully tuning _λ_, practitioners can navigate the performance-interpretability frontier,
thereby producing policies that respect human cognitive constraints while upholding
a high level of decision-making quality.

**3.4 Quality-Diversity Archive**

To augment the diversity and comprehensiveness of the evolved solutions, our evolutionary model relies on an archive based on Quality-Diversity (QD) principles. This
QD archive serves the purpose of maintaining a collection of high-performing yet
behaviorally and structurally diverse policies, thereby allowing exploration over a wide
range of policies during the evolution of solutions. This type of strategy significantly
boosts the chances of discovering innovative and insightful solutions that are otherwise unavailable through standard evolutionary approaches or reinforcement learning
when employed in isolation.

_**Archive Structure.**_

Formally, the QD archive constitutes a highly structured multidimensional system
in which every dimension corresponds to a meticulously chosen descriptor covering
specific policy behaviors or structural features. In our analysis, we define two main
descriptor dimensions:

1. **Policy Length (Complexity):** The total number of rules in the policy, which
correlates directly with its level of interpretability and complexity.
2. **Average Predicate Threshold (Behavioral Characteristic):** The mean
threshold used in predicates for important state dimensions. This feature plays a
crucial role in capturing subtle yet important behavioral differences among policies,
enabling the distinction between decision-making strategies stored in the archive.

10

Each cell in this grid stores the best-performing policy for the given descriptor
coordinates, thus providing a systematic and effective representation of diversity.

_**Policy Descriptor Calculation.**_

Specifically, given a policy _π_ with _k_ rules, the descriptor vector **d** ( _π_ ) is computed as
follows:
**d** ( _π_ ) = [ _k,_ _th_ [¯] ] _,_ (11)

where the first component _k_ is the total number of rules, and the second component _th_ [¯]
is the average predicate threshold value across all relevant predicates, formally defined

as:


_m_ _i_
� _|th_ _i,j_ _|,_ (12)

_j_ =1


¯ 1
_th_ =
_N_ pred


_k_
�

_i_ =1


with _th_ _i,j_ representing the threshold of the _j_ [th] predicate in rule _R_ _i_, and _N_ pred =
� _ki_ =1 _[m]_ _[i]_ [ is the total number of predicates in the policy.]

_**Archive Update Mechanism.**_

The archive is constantly updated in line with the evolution’s developments based
on assessments of the policies’ fitness. For every proposed policy _π_, we calculate the
descriptor vector **d** ( _π_ ) and assign it a unique location in the archive grid. The policy
is added in the corresponding cell if one or more of the following conditions are met:

- The cell is empty, indicating that no policy currently occupies that behavioral and
structural niche.

- The candidate policy’s fitness exceeds that of the policy currently occupying the
cell, thus representing a progress in the given descriptor space.

Formally, let _Q_ denote the archive grid, indexed by descriptor coordinates **d**, and
let _Q_ [ **d** ] denote the policy stored at coordinates **d** . Then, the archive update rule is:


_π_ if _Q_ [ **d** ( _π_ )] is empty,


_Q_ [ **d** ( _π_ )] =





_π_ if Fitness( _π_ ) _>_ Fitness( _Q_ [ **d** ( _π_ )]) _,_


(13)


_Q_ [ **d** ( _π_ )] otherwise _._


_**Advantages of the QD Archive.**_

The deliberate introduction of a Quality-Diversity archive has a number of important
advantages:

- **Enhanced Exploration:** The QD archive allows for continued exploration by preserving a diverse set of high-quality solutions across various important dimensions,
rather than rushing toward a small set of solutions.

- **Interpretability Spectrum:** Because policy complexity is a central feature,
stakeholders enjoy direct access to a range of solutions extending from simple, easyto-understand policies to more complicated ones, thus allowing the flexibility to
choose solutions that match specific requirements in the domain.

11

- **Behavioral Insight:** The use of predicate threshold descriptors to capture behavioral subtleties enables a deeper understanding of policy behaviors, allowing
practitioners to better understand and analyze decision-making approaches that
have evolved within the framework.

Finally, the incorporation of the Quality-Diversity archive in our genetic rule-based
reinforcement learning system not only significantly improves the degree of exploration of the search space but also greatly enhances the diversity, interpretability, and
practical usability of the resulting solutions.

**3.5 Full Algorithm Description**

Here, we detail the overall workflow of BASIL, bringing together all the main
elements—policy representation, genetic operators, fitness function, and QualityDiversity (QD) archiving—into a single cohesive algorithm.
Algorithm 1 encapsulates the full evolutionary process of our method. Starting from
a diverse initial population, the algorithm proceeds through successive generations
of evaluation, selection, variation, and archiving. Each component is designed to
enforce a dual objective: maintain policy _performance_ while guaranteeing _interpretabil-_
_ity_ through explicit complexity control. The Quality-Diversity archive further ensures
that diverse behavioral and structural niches are preserved and optimized across generations, resulting in a robust portfolio of symbolic, transparent, and high-quality
decision policies.

12

**Algorithm 1** BASIL
**Require:** Population size _N_, maximum generations _G_, number of evaluation episodes
_E_, complexity penalty coefficient _λ_, mutation probability _p_ mut, crossover probability _p_ cross, archive descriptor mapping _d_ ( _π_ )
**Ensure:** Final QD archive _Q_ containing diverse, high-performing, and interpretable
policies

1: Initialize a population _P_ of _N_ random rule-based policies

2: Initialize empty Quality-Diversity archive _Q_

3: **for** _g_ = 1 to _G_ **do**

4: **for all** _π ∈P_ **do**

5: Evaluate _π_ over _E_ episodes to compute average return

6: Compute complexity of _π_ (total number of predicates)

7: Compute fitness: Fitness( _π_ ) = Performance( _π_ ) _−_ _λ ·_ Complexity( _π_ )

8: Map _π_ to descriptor coordinates _d_ ( _π_ ) and update _Q_ if:

9: cell _Q_ [ _d_ ( _π_ )] is empty **or**

10: Fitness( _π_ ) _>_ Fitness( _Q_ [ _d_ ( _π_ )])

11: **end for**

12: Select top _K_ elite policies from _Q_

13: Initialize new population _P_ _[′]_ _←_ elites

14: **while** _|P_ _[′]_ _| < N_ **do**

15: Randomly select parents _π_ 1 _, π_ 2 from elites

16: **if** random number _< p_ cross **then**

17: _π_ child _←_ Crossover( _π_ 1 _, π_ 2 )

18: **else**

19: _π_ child _←_ copy of randomly chosen parent

20: **end if**

21: **if** random number _< p_ mut **then**

22: _π_ child _←_ Mutate( _π_ child )

23: **end if**

24: Add _π_ child to _P_ _[′]_

25: **end while**

26: Update population: _P ←P_ _[′]_

27: **end for**

28: **return** final archive _Q_
### **4 Experiments & Results**

To test the effectiveness, robustness, and generalizability of our symbolic rule-based
genetic reinforcement learning methodology, we performed a rigorous set of experiments on three well-known continuous-state benchmark domains in the OpenAI Gym
library: **CartPole-v1**, **MountainCar-v0**, and **Acrobot-v1** . These environments
have their own set of challenges in terms of dynamics, control complexity, and reward
sparsity, thus providing a diversified and challenging testbed for our methodology.

13

The experiments presented here are designed to demonstrate the ability of our
algorithm to obtain highly interpretable policies for solving challenging reinforcement
learning problems. Furthermore, we present a systematic study of the effect of the
number of rules and predicates in a given policy on overall performance, allowing
for a quantifiable evaluation of the trade-off between reward maximization and interpretability. Another important direction of analysis involves the diversity of evolved
behaviors across different policy structures. In order to increase the tractability of this
study, we utilize a Quality-Diversity (QD) approach, where solutions are encoded in
a structured descriptor space that includes both behavioral and structural features,
i.e., the mean angle thresholds and overall number of rules. This approach allows us
to visualize and explore the complexity of the solution set that was found.

**4.1 Experimental Setup**

Each environment was treated using the same general evolutionary model outlined
in Section 3. An initial population of 200 policies, represented as symbolic if-then
conditions, was randomly constructed. Policies in each generation were ranked based
on their mean episodic return over 5 episodes, with penalties given as a weighted sum
of the predicates used. The top 25% of the best performers were selected for inclusion
in the next generation through crossover and mutation operations. Evolution was
continued for up to 500 generations or was terminated early with the discovery of a
near-optimal policy.
Policies are not only judged based on their effectiveness but also on how succinctly
they represent behavior. Having fewer predicates makes the policy more interpretable
for humans, a critical component of our research statement. For every experimental
design, we used a Quality-Diversity (QD) archive in order to ensure a diversity of
policies in a discretized set of descriptors. Descriptors were derived from important
behavioral metrics such as the mean pole angle for _CartPole_, the horizontal position
for _MountainCar_, and the angular trajectory for _Acrobot_ .
All experiments were run using the same hyperparameters (namely, mutation and
crossover probabilities, maximum rules per policy, and evaluation episodes) to ensure
fair conditions and to test the method’s flexibility across different tasks without requiring manual tuning. To ensure robustness and account for stochastic variability, each
experiment was repeated using different random seeds.
In the following subsections, we summarize in-depth findings and assessments
relevant to the respective environment.

**4.2 CartPole-v1 Results**

The empirical evaluation of BASIL begins with the `CartPole-v1` environment, a
standard benchmark for discrete control problems. The main goal consists of coming
up with a policy for keeping a pole in balance that is placed on a cart by exerting discrete forces either to the left or right. Despite its simplicity, `CartPole-v1`
remains a great testing environment for measuring the effectiveness, generalizability,
and interpretability of reinforcement learning methods.

14

This algorithm has shown that it can attain near-optimal performance on the target
task in a small number of generations. On a typical experimental run, it discovered a
strategy that consistently obtained a maximum reward of 500 _._ 0 in just five generations.
This entire experiment ran in under 4 seconds on a standard desktop CPU, with both
sample efficiency and computational efficiency being evident here. In its expression,
this strategy was conveyed by a single symbolic rule using just two logical predicates.
As a result, the produced controller is characterized by both high performance and
_high interpretability_, reflecting the strengths of our genetic-symbolic approach.

**Best policy:**

_If_ _s_ [2] _> −_ 0 _._ 02 _and_ _s_ [3] _> −_ 0 _._ 30, _then action = 1_ ; _else action = 0_ .

This concise guideline captures an important principle in the domain of CartPole:
corrective action should be taken when the pole angle ( _s_ [2]) moves past a small threshold and the pole’s angular velocity ( _s_ [3]) is positive at the moment. This effect exists
outside of the shaping or prior knowledge requirements and was exclusively discovered
through evolutionary search in the logical rule space.

The success of such a minimalist policy illustrates a number of key advantages with
regard to our model:

- **Performance** : The policy successfully realized its purpose by earning the _maximum_
_reward_ of 500 _._ 0 in the test episodes. Its recorded fitness value was slightly less than
500, due to the complexity penalty accumulated from its two logical predicates, but
it does not represent a performance deficit.

- **Interpretability** : The final policy is fully transparent and interpretable by humans,
based on one single rule and two predicates.

- **Efficiency** : The policy was implemented in just five generations and a short period
of time, requiring very little computational resources and information.

For the purpose of comparison, a baseline Deep Q-Network (DQN) was trained in
parallel with the same objective while deploying a traditional neural model. While
correlated with success in modern literature, the DQN required significantly more
time, memory, and computational capabilities. The greedy policy reached a reward
of 500 _._ 0 after 360 episodes, with a total training time of 63.4 seconds—more than 15
times slower than the model proposed in this study. Additionally, the policy learned
with the DQN forms a black-box neural model with over 30,000 parameters, thereby
being fully opaque and difficult—if not impossible—to interpret or verify.

These results highlight the profound tradeoff between our symbolic framework and
conventional deep RL methods. While DQN achieves high reward, it does so at the cost
of interpretability, training time, and architecture complexity. In contrast, our method
delivers human-understandable policies in orders of magnitude less time—without
sacrificing performance.

Moreover, the Quality-Diversity archive captured a range of approaches with high
effectiveness and behavioral diversity. These approaches can be used for follow-up
tasks such as policy distillation, ensemble decision-making, or interpretability analysis.

15

Overall, the experience with the CartPole experiment presents convincing evidence
for the success of our approach in automatically generating concise, high-performing
policies through symbolic representation.

**4.3 MountainCar-v0 Results**

We now turn our attention to the `MountainCar-v0` environment, a classic benchmark
that is a much more difficult task than CartPole. The agent must learn to escape a
steep valley by gaining sufficient momentum to climb the hilltop. Success in this task
requires long-term planning and skilful handling of sparse reward signals—situations
where many standard approaches, particularly those grounded in gradients, tend to
struggle to succeed reliably.
The use of the symbolic-genetic algorithm in this research proved highly effective,
resulting in a compact and transparent controller solving the environment in a remarkably low number of generations. For a single trial run, the policy had a mean reward
of _−_ 88 _._ 7, significantly above the generally accepted success threshold of _−_ 110 _._ 0. The
total run time was around 38 seconds, with the algorithm reaching convergence in
only 7 generations.

**Best policy:**

_If_ _s_ [1] _>_ 0 _._ 00, _then action = 2_ ;
_else if_ _s_ [0] _>_ 0 _._ 30 _∧_ _s_ [0] _<_ 0 _._ 60 _∧_ _s_ [1] _> −_ 0 _._ 07, _then action = 0_ ;
_else if_ _s_ [0] _>_ 0 _._ 30, _then action = 2_ ;
_else if_ _s_ [0] _<_ 0 _._ 60, _then action = 0_ ;
_else if_ _s_ [1] _> −_ 0 _._ 03, _then action = 2_ ;
_else action = 0_ .

This five-rule policy encodes interpretable momentum-based strategies, with rules
corresponding directly to the agent’s position and velocity. Impressively, the policy has
just 5 symbolic rules and 7 predicates, is completely human-readable, and nonetheless
exhibits strong and generalizable behavior in evaluation episodes.

- **Performance:** Our approach attained an average evaluation reward of _−_ 88 _._ 7,
reflecting successful completion of the task with large margin above the _−_ 110
threshold.

- **Interpretability:** The learned policy remains completely transparent, and every
action can be traced back to a logical rule with physically meaningful thresholds.

- **Efficiency:** Convergence was attained in less than 40 seconds using only evolutionary logic and no gradients, backpropagation, or complicated model updates.

**Comparison with DQN.** We compared our approach with a baseline Deep QNetwork (DQN) implementation using _ϵ_ -greedy exploration and periodic greedy
evaluations. DQN eventually cleared the success threshold after 460 episodes at an
average evaluation reward of _−_ 108 _._ 2. Overall training time was considerably longer,
however—more than 184 seconds—and the policy produced is embedded in a neural
network and is thus totally impenetrable to interpretation and hard to verify or debug.
Conversely, our approach yielded a symbolic controller that is _orders of magnitude_

16

_more interpretable_, almost twice as quick to train, and ultimately more adaptable to
transparent decision-making.

These results reinforce the viability of symbolic policy evolution for intricate control domains. Our algorithm delivers not just performance-competitive outcomes but
also high levels of transparency, an essential element in safety-critical or real-world
applications.

**4.4 Acrobot-v1 Results**

We complete our evaluation with the `Acrobot-v1` environment, a traditional benchmark for underactuated control. The task is to swing the lower link of a two-joint
pendulum upward so that the tip of the bottom link reaches a target height. With a
six-dimensional continuous state space, sparse delayed rewards, and nonlinear dynamics, Acrobot is a difficult task for policy synthesis—especially when interpretability is
a design constraint.
Despite this complexity, our symbolic-genetic algorithm evolved a high-performing
controller encoded with only 3 rules and 7 logical predicates. During a typical run,
the best-evolved policy received an average evaluation reward of _−_ 75 _._ 1, and its final
execution received a reward of _−_ 65 _._ 0. This performance exceeds that of typical baseline heuristics and rivals that of gradient-based neural methods—with the additional
advantage of transparency and ease of inspection.

**Best policy:**

_If_ _s_ [3] _< −_ 4 _._ 00 _and_ _s_ [5] _> −_ 4 _._ 00, _then action = 2_ ;
_else if_ _s_ [5] _>_ 0 _._ 00 _and_ _s_ [4] _<_ 0 _._ 50 _and_ _s_ [5] _>_ 0 _._ 00, _then action = 2_ ;
_else if_ _s_ [4] _> −_ 0 _._ 50 _and_ _s_ [3] _>_ 0 _._ 00, _then action = 0_ ;
_else action = 0_ .

Each rule corresponds to a distinct behavioral motif: ramping up speed with cooperative acceleration, adapting to angular velocity constraints, and stabilizing near the
target area. These interpretable policies were not hand-designed but rather discovered through evolutionary search over rule-based programs—without gradients, reward
shaping, or pretraining in general.

- **Performance** : The symbolic policy had an average reward of _−_ 75 _._ 1 and achieved
_−_ 65 _._ 0 in its best rollout—competitive in this domain.

- **Interpretability** : The controller architecture is concise yet adequate, with three
comprehensible rules directly capturing velocity and angle interactions.

- **Efficiency** : Convergence was achieved in under 90 seconds of CPU time, with no
backpropagation or neural function approximation.

**Comparison with DQN.** To provide additional context, we also trained a Deep
Q-Network baseline in the same settings. DQN’s policy took more than 460 episodes
and 508.6 training seconds to succeed, at a mean greedy evaluation reward of _−_ 94 _._ 2.
Although the black-box controller ultimately succeeded, it was behind our approach in
both efficiency and transparency. Unlike our symbolic controller, which was developed

17

via evolution, DQN’s learned policy is entrenched in thousands of unclear parameters
and does not give any indication of its decision-making process.

In short, not only is BASIL a viable solution to the Acrobot problem, but one that
is also interpretable, in addition to being robust, minimal, and general. Our method’s
success on this domain suggests that symbolic policies, evolved through qualitydiversity optimization, can hold their own against deep learning counterparts—even
in high-dimensional, non-linear worlds.

**Table 1** Comparison of Our Symbolic Method vs. DQN Across Three Benchmarks

|Environment|Approach|Avg Reward|Training Time (s)|Interpretability|
|---|---|---|---|---|
|CartPole-v1|Ours<br>DQN|500.0<br>500.0|< 4<br>63.4|Yes (1 rule, 2 predicates)<br>No|
|MountainCar-v0|Ours<br>DQN|−88.7<br>−108.2|< 40<br>184.1|Yes (5 rules, 7 predicates)<br>No|
|Acrobot-v1|Ours<br>DQN|−75.1<br>−94.2|< 90<br>508.6|Yes (3 rules, 7 predicates)<br>No|

### **5 Conclusion**

The pursuit of interpretable reinforcement learning has long been hindered by a
fundamental trade-off: transparency often comes at the cost of performance, and performance at the cost of interpretability. In this work, we challenged that dichotomy.
Through BASIL—Best-Action Symbolic Interpretable Learning—we presented not
just a new method, but a new philosophy for reinforcement learning: one that treats
interpretability not as a constraint, but as a core design principle and source of
strength.
BASIL departs from the opaque architectures of deep neural policies and enters
the symbolic domain, where rules are readable, decisions are traceable, and behavior
is meaningful. By evolving rule-based policies in an online, environment-interactive
manner, guided by a carefully designed complexity-aware fitness function and a
Quality-Diversity archive, we show that high performance and full transparency
can be achieved simultaneously—without approximation, post-hoc rationalization, or
gradient-based tuning.
Across a range of control tasks— `CartPole-v1`, `MountainCar-v0`, and
`Acrobot-v1` —BASIL consistently evolved compact symbolic controllers that matched
or exceeded the performance of deep Q-networks, but did so orders of magnitude
faster and with policies that can be written, read, and explained in a single sentence.
This is not just a result; it is a paradigm shift. It shows that reinforcement learning
agents can be not only intelligent, but also inherently understandable, auditable, and
aligned with human reasoning.

18

The implications of this shift are profound. In safety-critical domains such as
autonomous driving, healthcare, finance, or policy governance, the need for transparent decision-making is not a luxury—it is a necessity. Our approach speaks directly
to these domains, offering decision policies that are not just effective, but legible; not
just optimized, but trustworthy.
BASIL’s policy representation—ordered logical rule-lists over thresholded state
predicates—brings multiple advantages. First, it enables exact tracing of behavior
under all possible conditions. Second, it allows domain experts to verify, audit, or
even manually edit evolved policies. Third, it allows the learning process to be constrained or biased toward domain knowledge in ways that are fundamentally impossible
for opaque models. And finally, it enables future integration with formal verification
pipelines, symbolic planners, or high-assurance control systems.
The quality-diversity mechanism further elevates our framework. Rather than converging prematurely to a single high-performing policy, BASIL maintains a diverse
collection of distinct policies—each representing a viable strategy in the solution space.
This supports multiple practical applications: fallback safety mechanisms, ensemble
decision-making, adaptive control under regime shifts, or pedagogical presentation of
alternative strategies to human users.
Unlike many interpretable RL methods that require offline data, pre-trained oracles, or post-hoc approximators, BASIL learns from scratch, online, and within the
environment itself. It does not extract rules from a black-box model—it never builds

one. The policies it evolves are the only model it uses. This directness is powerful: it
means that every action taken by the agent, at every point in its learning trajectory, is
governed by a symbolic structure that is always available for inspection, intervention,
or understanding.
What we demonstrate through BASIL is that compactness and capability are not
mutually exclusive. On the contrary, compactness can drive generalization. Because
our rules are minimal, they avoid overfitting. Because they are symbolic, they generalize across state variations more naturally. Because they are interpretable, they enable
manual debugging, validation, or refinement. These are not traits of fragile heuristic
systems; they are hallmarks of robust, general-purpose intelligence.
Equally important is the computational footprint of our method. On standard
hardware, BASIL finds optimal policies for classical control tasks in seconds—not
hours. It uses no backpropagation, no gradient descent, no complex network optimization. It uses evolution: a population of candidate policies, genetic variation, and
selective pressure toward performance and parsimony. In doing so, it demonstrates
that powerful, scalable, and interpretable learning is possible without high resource
consumption or black-box dependencies.
Beyond its empirical success, BASIL invites a richer research agenda—one that
bridges learning with logic, evolution with explanation, and performance with principle. It opens new frontiers in hybrid neuro-symbolic systems, program synthesis,
curriculum-driven symbolic evolution, and formal verification of policies. Its modular architecture enables easy integration with perception modules, symbolic planners,
and constraint-based solvers. It could serve as the core of explainable robotics,

19

safe autonomous agents, or interactive learning systems where transparency is
non-negotiable.
Perhaps most fundamentally, BASIL reasserts a core belief: that machine intelligence should not just aim to be effective, but also intelligible. In a world increasingly
shaped by AI systems, trust is not earned by performance alone. It is earned through
clarity, comprehensibility, and alignment with human expectations. Our method
speaks to that imperative.
We are not the first to argue that interpretable models matter. But with BASIL,
we go further: we show that interpretability can be evolved, not engineered; inherent, not imposed; and essential, not optional. In doing so, we provide not just a
tool for researchers, but a blueprint for a new generation of transparent learning
systems—systems that are fast, capable, and always explainable.
The future of reinforcement learning does not lie in larger black boxes. It lies in
clarity. It lies in methods like BASIL, where every decision has a reason, every action
has a logic, and every policy is a story we can read. As we move toward human-aligned
AI, symbolic policy learning may not just be an alternative—it may be the path we
need.
### **References**

[1] Sutton, R.S., Barto, A.G.: Reinforcement Learning: An Introduction, 2nd edn.
MIT Press, Cambridge, MA (2018)

[2] Kaelbling, L.P., Littman, M.L., Moore, A.W.: Reinforcement learning: A survey.
Journal of Artificial Intelligence Research **4**, 237–285 (1996)

[3] Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.A., Veness, J., Bellemare, M.G.,
Graves, A., Riedmiller, M., Fidjeland, A.K., Ostrovski, G., Petersen, S., Beattie,
C., Sadik, A., Antonoglou, I., King, H., Kumaran, D., Wierstra, D., Legg, S.,
Hassabis, D.: Human-level control through deep reinforcement learning. Nature
**518** [, 529–533 (2015) https://doi.org/10.1038/nature14236](https://doi.org/10.1038/nature14236)

[4] Schulman, J., Wolski, F., Dhariwal, P., Radford, A., Klimov, O.: Proximal
policy optimization algorithms. In: arXiv Preprint arXiv:1707.06347 (2017).
https://arxiv.org/abs/1707.06347

[5] Haarnoja, T., Zhou, A., Abbeel, P., Levine, S.: Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. In: Proceedings of the 35th International Conference on Machine Learning (ICML)
(2018). https://arxiv.org/abs/1801.01290

[6] Rudin, C.: Stop explaining black box machine learning models for high stakes
decisions and use interpretable models instead. Nature Machine Intelligence **1** (5),
[206–215 (2019) https://doi.org/10.1038/s42256-019-0048-x](https://doi.org/10.1038/s42256-019-0048-x)

[7] Doshi-Velez, F., Kim, B.: Towards a rigorous science of interpretable machine

20

learning. arXiv preprint arXiv:1702.08608 (2017)

[8] Such, F.P., Madhavan, V., Conti, E., Lehman, J., Stanley, K.O., Clune, J.: Deep
neuroevolution: Genetic algorithms are a competitive alternative for training deep
neural networks for reinforcement learning. In: arXiv Preprint arXiv:1712.06567
(2017). https://arxiv.org/abs/1712.06567

[9] Koza, J.R.: Genetic Programming: On the Programming of Computers by Means
of Natural Selection. MIT Press, Cambridge, MA (1992)

[10] Urbanowicz, R.J., Moore, J.H.: Learning classifier systems: A complete introduction, review, and roadmap. Journal of Artificial Evolution and Applications **2009**,
[1–25 (2009) https://doi.org/10.1155/2009/736398](https://doi.org/10.1155/2009/736398)

[11] Hein, D., Udluft, S., Runkler, T.: Generating interpretable reinforcement learning policies using genetic programming. In: Proceedings of the Genetic and
[Evolutionary Computation Conference Companion, pp. 958–965 (2019). https:](https://doi.org/10.1145/3319619.3326811)
[//doi.org/10.1145/3319619.3326811](https://doi.org/10.1145/3319619.3326811)

[12] Mitchell, M.: An Introduction to Genetic Algorithms. MIT Press, Cambridge,
MA (1998)

[13] Pugh, J.K., Soros, L.B., Stanley, K.O.: Quality diversity: A new frontier for
evolutionary computation. Frontiers in Robotics and AI **3** [, 40 (2016) https:](https://doi.org/10.3389/frobt.2016.00040)
[//doi.org/10.3389/frobt.2016.00040](https://doi.org/10.3389/frobt.2016.00040)

[14] Custode, L.L., Iacca, G.: Evolutionary learning of interpretable decision trees.
IEEE Access **8** [, 177437–177449 (2020) https://doi.org/10.1109/ACCESS.2020.](https://doi.org/10.1109/ACCESS.2020.3027091)
[3027091](https://doi.org/10.1109/ACCESS.2020.3027091)

[15] Cully, A., Clune, J., Tarapore, D., Mouret, J.-B.: Robots that can adapt like animals. Nature **521** [(7553), 503–507 (2015) https://doi.org/10.1038/nature14422](https://doi.org/10.1038/nature14422)

[16] Ferigo, A., Custode, L.L., Iacca, G.: Quality diversity evolutionary learning of
decision trees. In: Proceedings of the 38th ACM/SIGAPP Symposium on Applied
[Computing, pp. 425–432 (2023). https://doi.org/10.1145/3555776.3577591](https://doi.org/10.1145/3555776.3577591)

[17] Custode, L.L., Iacca, G.: A co-evolutionary approach to interpretable reinforcement learning in environments with continuous action spaces. In: 2021 IEEE
[Symposium Series on Computational Intelligence (SSCI), pp. 1–8 (2021). https:](https://doi.org/10.1109/SSCI50451.2021.9659837)
[//doi.org/10.1109/SSCI50451.2021.9659837](https://doi.org/10.1109/SSCI50451.2021.9659837)

[18] Zhang, H., Lin, X., Sun, Y., Jin, Y.: Interpretable policy derivation for reinforcement learning based on evolutionary feature synthesis. Complex & Intelligent
Systems **6** [(3), 545–557 (2020) https://doi.org/10.1007/s40747-020-00156-6](https://doi.org/10.1007/s40747-020-00156-6)

21

[19] Verma, A., Singh, R., Chaudhuri, S.: Programmatically interpretable reinforcement learning. In: Proceedings of the 35th International Conference on Machine
[Learning, pp. 5045–5054 (2018). https://doi.org/10.48550/arXiv.1804.02477](https://doi.org/10.48550/arXiv.1804.02477)

[20] Ma, Y., Fan, Y., Wang, Y., Zhang, Z., Zhan, Y., Jin, Y.: Learning neural-symbolic
policies with graph neural networks. In: Proceedings of the 30th International
[Joint Conference on Artificial Intelligence (IJCAI), pp. 2433–2439 (2021). https:](https://doi.org/10.24963/ijcai.2021/335)
[//doi.org/10.24963/ijcai.2021/335](https://doi.org/10.24963/ijcai.2021/335)

[21] Hein, D., Udluft, S., Runkler, T.: Generating interpretable fuzzy controllers using
particle swarm optimization and genetic programming. In: Proceedings of the
Genetic and Evolutionary Computation Conference Companion, pp. 139–140
[(2018). https://doi.org/10.1145/3205651.3208253](https://doi.org/10.1145/3205651.3208253)

[22] Babuˇska, R.: Genetic programming methods for reinforcement learning. In:
Proceedings of the Genetic and Evolutionary Computation Conference, pp.
[1389–1390 (2019). https://doi.org/10.1145/3319619.3326845](https://doi.org/10.1145/3319619.3326845)

[23] Liventsev, V., H¨arm¨a, A., Petkovi´c, M.: Bf++: a language for general-purpose
program synthesis. arXiv preprint arXiv:2101.09571 (2021)

[24] Gevaert, A., Davis, J., Saeys, Y.: Distilling deep rl models into interpretable
neuro-fuzzy systems. In: 2022 IEEE International Conference on Fuzzy Systems
[(FUZZ-IEEE), pp. 1–8 (2022). https://doi.org/10.1109/FUZZ-IEEE52011.2022.](https://doi.org/10.1109/FUZZ-IEEE52011.2022.9857362)
[9857362](https://doi.org/10.1109/FUZZ-IEEE52011.2022.9857362)

[25] Landajuela, M., Heide, F., Kolter, Z.: Discovering symbolic policies with deep
reinforcement learning. In: Advances in Neural Information Processing Systems
[(NeurIPS), pp. 1–12 (2021). https://doi.org/10.48550/arXiv.2106.11299](https://doi.org/10.48550/arXiv.2106.11299)

22

