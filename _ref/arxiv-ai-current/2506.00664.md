1

# OntoRAG: Enhancing Question-Answering through Automated Ontology Derivation from Unstructured Knowledge Bases

Yash Tiwari, Owais Ahmad Lone, Mayukha Pal*


_**Abstract**_ **—Ontologies are pivotal for structuring knowledge**
**bases to enhance question-answering (QA) systems powered by**
**Large Language Models (LLMs). However, traditional ontology**
**creation relies on manual efforts by domain experts, a process**
**that is time-intensive, error-prone, and impractical for large,**
**dynamic knowledge domains. This paper introduces OntoRAG,**
**an automated pipeline designed to derive ontologies from un-**
**structured knowledge bases, with a focus on electrical relay**
**documents. OntoRAG integrates advanced techniques, includ-**
**ing web scraping, PDF parsing, hybrid chunking, information**
**extraction, knowledge graph construction, and ontology cre-**
**ation, to transform unstructured data into a queryable ontol-**
**ogy. By leveraging LLMs and graph-based methods, OntoRAG**
**enhances global sensemaking capabilities, outperforming conven-**
**tional Retrieval-Augmented Generation (RAG) and GraphRAG**
**approaches in comprehensiveness and diversity. Experimental**
**results demonstrate OntoRAG’s effectiveness, achieving a com-**
**prehensiveness win rate of 85% against vector RAG and 75%**
**against GraphRAG’s best configuration. This work addresses the**
**critical challenge of automating ontology creation, advancing the**
**vision of the semantic web.**

_**Index Terms**_ **—Ontology Learning, Retrieval-Augmented Gen-**
**eration, Knowledge Graphs, Large Language Models**

I. I NTRODUCTION

Large Language Models (LLMs) have achieved unprecedented success in natural language processing, demonstrating remarkable capabilities across a wide range of tasks

[1]–[3]. Despite these advancements, LLMs face significant
challenges in domain-specific, knowledge-intensive applications, particularly when queries extend beyond their training
data or require up-to-date information. A primary limitation is hallucination—the generation of factually incorrect
responses—stemming from the models’ inability to access
external knowledge dynamically [4], [5]. Retrieval-Augmented
Generation (RAG) addresses this issue by integrating external knowledge bases into the generation process, retrieving
relevant documents through semantic similarity to enhance
factual accuracy [6]. This approach has become a cornerstone
for improving LLM-based question-answering (QA) systems,

Mr. Yash Tiwari is with ABB Ability Innovation Center, Bangalore -560058,
IN, working as R&D Data Scientist.
Mr. Owais Ahmad Lone is a Data Science Research Intern at ABB Ability
Innovation Center, Hyderabad 500084, India, and also a B.Tech student at
the Department of Computer Science and Engineering, Indian Institute of
Technology, Kharagpur, West Bengal, 721302, IN.
Dr. Mayukha Pal is with ABB Ability Innovation Center, Hyderabad500084, IN, working as Global R&D Leader – Cloud & Advanced Analytics
(e-mail: mayukha.pal@in.abb.com).


enabling real-world applications that demand current and
precise information.
Traditional RAG relies on vector search, which encodes
text chunks into vector representations and retrieves those
most semantically similar to a query. While effective for
localized retrieval, this method struggles with global sensemaking tasks, such as Query-Focused Summarization, where
relevant information is dispersed across multiple documents
or regions within a corpus [7]. Vector search focuses on
individual text chunks in isolation, failing to capture the
interconnected relationships and broader context necessary
for comprehensive understanding. For instance, in technical
domains like electrical relays, a query about failure modes may
require synthesizing information from specifications, manuals,
and datasheets—information that is often fragmented and
relationally complex. Moreover, vector search lacks the ability
to perform multi-hop reasoning, a critical requirement for
answering complex queries that depend on traversing relationships between entities across a knowledge base [18]–[20].
To address these shortcomings, Microsoft Research introduced GraphRAG, which leverages knowledge graphs to
enable more holistic information retrieval [8]. In a knowledge
graph, entities are represented as nodes and their relationships
as typed edges, facilitating multi-hop reasoning over interconnected data. GraphRAG enhances global sensemaking by
using community detection to cluster the knowledge graph into
multiple communities, which are then used for summarization
and retrieval. However, a significant limitation of GraphRAG
is its approach to clustering: it directly creates multiple clusters
without preserving the ontological integrity of the source documents. This process often disregards the inherent hierarchical
relationships, entity classifications, and semantic dependencies
present in the original data, leading to a loss of structural and
semantic coherence in the resulting knowledge representation.
For example, in the electrical relay domain, critical relationships—such as those between relay components and their
failure modes—may be fragmented across clusters, distorting
the knowledge base’s integrity and reducing the accuracy of
question-answering tasks. Additionally, GraphRAG typically
assumes the availability of a pre-existing ontology to guide
the structuring of the knowledge graph, which requires manual
curation—a labor-intensive, costly, and error-prone process
that limits scalability and adaptability to new domains [15],

[16], [21].
Ontologies are foundational to the semantic web, a vision
articulated by Tim Berners-Lee to enable machine-processable

data semantics for automated services [22]. They facilitate
data integration, sharing, and discovery by defining reusable
classes, properties, and relationships, as exemplified by manually curated ontologies like the Unified Medical Language
System (UMLS), WordNet, GeoNames, and the Dublin Core
Metadata Initiative [10]–[13]. However, the manual effort
required to create such ontologies is increasingly untenable
in the face of massive, dynamic knowledge bases, where
information standardization is a rapidly evolving challenge

[14]. Automating ontology creation is thus a critical research
frontier, with recent efforts exploring LLM-based approaches
for ontology learning (OL) [17]. These approaches aim to
scale ontology construction by leveraging the semantic understanding capabilities of LLMs, but they often lack the
structural reasoning needed to fully capture the hierarchical
and relational nature of knowledge bases while preserving their
ontological integrity.
This paper introduces OntoRAG, an automated pipeline
designed to derive ontologies from unstructured knowledge
bases, addressing the limitations of both traditional vector
search and GraphRAG. With a focus on electrical relay documents—a domain where information is scattered across tech
nical manuals, datasheets, and application notes—OntoRAG
transforms unstructured PDFs into a queryable ontology
through a series of stages: web scraping, PDF parsing, hybrid
chunking, information extraction, knowledge graph construction, and ontology creation. Unlike vector RAG, OntoRAG
captures global relationships and hierarchical structures by
deriving ontology classes and relationships, enabling multihop reasoning and comprehensive sensemaking. In contrast to
GraphRAG, OntoRAG preserves the ontological integrity of
the source documents by explicitly deriving an ontology that
reflects the inherent hierarchical and relational structure of the
data. It employs community detection algorithms alongside
LLM-based property synthesis (via Gemini 2.5 Flash) to
ensure that ontology classes, relationships, and properties align
with the semantic and structural coherence of the original
documents, resulting in a more faithful and meaningful knowledge representation. This automated ontology creation process
eliminates the need for manual curation, enhancing scalability
and adaptability across diverse domains. By integrating LLMs
with graph-based techniques, OntoRAG balances semantic
understanding with structural reasoning, achieving superior
performance in key metrics such as comprehensiveness and
diversity.
The primary contributions of this work are threefold: 1)
An end-to-end pipeline for automated ontology creation from
unstructured data, eliminating the manual effort required by
traditional methods and GraphRAG while preserving ontological integrity. 2) Novel techniques for PDF parsing, hybrid and
semantic chunking, and ontology derivation using community
detection and LLM-based property synthesis, tailored to handle complex, technical documentation. 3) A rigorous evaluation in the electrical relay domain, demonstrating OntoRAG’s
superiority over traditional and Graph RAG.
By addressing the gaps in traditional vector search and
GraphRAG, OntoRAG advances the vision of the semantic web, providing a scalable solution for automated ontol

2

ogy creation and enhanced question-answering in knowledgeintensive domains.

II. M ETHODOLOGY

OntoRAG is a systematic pipeline designed to transform unstructured PDF documents into a structured, queryable ontology. The pipeline comprises six key stages: web scraping, PDF
parsing, chunking, information extraction, knowledge graph
construction, and ontology creation. Each stage is meticulously
crafted to preserve semantic integrity while ensuring efficient
processing. The overall workflow is illustrated in Fig. 2.

_A. Web Scraping_

The OntoRAG pipeline begins with web scraping to acquire
a diverse set of electrical relay documents in PDF format.
Using tools such as BeautifulSoup [23] and Scrapy [24], we
systematically extracted PDFs from manufacturer websites and
technical repositories. The process navigated challenges like
dynamic web pages and web crawling to ensure robust data
collection. Scraped documents were organized into a centralized repository, facilitating efficient access for subsequent
processing stages.

_B. PDF Parsing_

_1) Parser Requirements:_ Effective PDF parsing is essential for enabling LLMs to process unstructured documents.
A robust parser must recognize document structures (e.g.,
paragraphs, tables, charts) and handle complex layouts, such
as multi-column pages and borderless tables. Electrical relay
documents often contain intricate layouts, including mixed
single- and double-column formats, tables with mathematical
equations, and embedded images, posing challenges for conventional rule-based parsers.
_2) Adoption of the Unstructured Library:_ To address these
challenges, we adopted the Unstructured library [25], which
excels in parsing PDFs into distinct elements such as titles,
headers, text, tables, and images. Unstructured combines Optical Character Recognition (OCR), object detection, and digital
parsing to accurately identify document layouts and element
locations. However, while Unstructured uses PDFMiner [26]
to extract text, it struggles to extract tabular structures despite
recognizing their presence.
_3) Enhanced Table Extraction:_ To improve table extraction,
we leveraged Unstructured’s coordinate information to isolate
table regions using PyMuPDF [27]. Given Unstructured coordinates ( _x_ 0 _, y_ 0 ) and ( _x_ 1 _, y_ 1 ) in units _U_, and PyMuPDF coordinates in units _P_, the transformed coordinates are computed

as:
_x_ 0 _· P_
( _X_ 0 _, Y_ 0 ) = _,_ _[y]_ [0] _[ ·][ P]_ _,_ (1)
� _U_ _U_ �


We applied padding (100 units vertically, 20 units horizontally)
to capture contextual elements like titles and footnotes:

( _X_ 0 _, Y_ 0 ) = ( _X_ 0 _−_ 20 _, Y_ 0 _−_ 100) _,_ (3)


_· P_

_,_ _[y]_ [0] _[ ·][ P]_
_U_ _U_


_U_


( _X_ 1 _, Y_ 1 ) = _x_ 1 _· P_
� _U_


_· P_

_,_ _[y]_ [1] _[ ·][ P]_
_U_ _U_


_,_ (1)
�

_._ (2)
�


_U_

3

Fig. 1. Overview of the Retrieval-Augmented Generation (RAG) process for question-answering, consisting of three main stages: Indexing, Retrieval, and
Generation.


Collect PDFs Extract text, tables, images Hybrid & semantic chunking


Extract entities, relationships Build graph






Fig. 2. Workflow of the OntoRAG pipeline, illustrating the transformation of unstructured PDF documents into a structured ontology through sequential
stages.


( _X_ 1 _, Y_ 1 ) = ( _X_ 1 + 20 _, Y_ 1 + 100) _._ (4)

Isolated table regions were processed using LLM. (see Appendix.)

_C. Chunking_

_1) Hybrid Chunking:_ The chunking stage divides parsed
content into manageable units for LLM processing while
preserving contextual integrity. We developed a hybrid
chunking technique using elements identified by the
Unstructured library. Chunks are created at Title elements,
provided the current chunk exceeds a minimum length
threshold (a hyperparameter). An upper length limit ensures
compatibility with LLM context windows, though exceptions
are made for single-element additions that exceed this limit.
This method ensures chunks are contextually coherent and
optimally sized.

_2) Semantic Chunking:_ To further enhance chunk quality,
we implemented semantic chunking as part of hybrid
chunking:

1) _Combination_ : Adjacent text chunks are combined with
their predecessors and successors to form combined

sentences.

2) _Sentence Embeddings_ : Combined sentences are embedded into vector representations using a text-embedding
model.

3) _Semantic Distances_ : Cosine distances between neighboring sentence embeddings are computed to measure
semantic similarity. Chunks with similarity above a
threshold are merged.
4) _Threshold Selection_ : The similarity threshold is determined by analyzing resultant chunk sizes and counts,
balancing granularity and coherence through manual
supervision.
This process produces _First Retrieval Granular Units_, ensuring chunks fit LLM context windows while maintaining
semantic coherence.

_D. Information Extraction and Mapping_

This stage transforms chunked data into structured atomic
facts, entities, and relationships for knowledge graph construction. The process involves several steps, all utilizing Gemini


2.5 Flash for in-context learning. The prompt is provided in
Appendix :

1) _Text Cleaning_ : Removes errors (e.g., spelling, grammar,
punctuation) to ensure data quality.
2) _Disambiguation_ : Resolves ambiguities (e.g., pronouns)
by replacing them with specific nouns, enhancing clarity.
3) _Named Entity Recognition (NER)_ : Identifies entities such
as names and technical specifications within chunks.
4) _Atomic Fact Extraction_ : Converts text into propositions
(subject-predicate-object statements), extracting key entities alongside each proposition. See Appendix.
5) _Chunk Graph JSON_ : Structures propositions into a
JSON object representing graph elements (nodes and
relationships), with nodes defined by properties (e.g.,
“part-of-speech”, “name”) and relationships by attributes
(e.g., “relationship-type”, “source node”, “target node”).
See Appendix.
6) _Key Element Mapping_ : Maps similar entities to global
canonical entities using a strict confidence threshold to
ensure consistency and reduce redundancy.
7) _Key Term Definitions and Embeddings_ : Generates definitions for key terms, embeds them into vectors, and adds
them to the chunk graph JSON for semantic retrieval.

_E. Knowledge Graph Construction_

In this stage, we construct a knowledge graph by clustering
key elements extracted from the text chunks into ontology
classes, which form the nodes of the graph. Relationships
between key elements are then projected onto the classes
to form edges, enabling multi-hop reasoning for ontology
creation.
Let _K_ = _{k_ 1 _, k_ 2 _, . . ., k_ _n_ _}_ be the set of key elements
extracted from the text chunks, where each _k_ _i_ has a name
embedding vec name ( _l_ _i_ ) _∈_ R _[d]_ and a definition embedding
vec def ( _l_ _i_ ) _∈_ R _[d]_ . We cluster these key elements into classes
using a disjoint-set data structure, based on their similarity in
name and definition embeddings. The similarity between two
key elements _k_ _i_ and _k_ _j_ is defined as:


sim( _k_ _i_ _, k_ _j_ ) =


1 if cos(vec name ( _k_ _i_ ) _,_ vec name ( _k_ _j_ )) _≥_ _θ_ name

and cos(vec def ( _k_ _i_ ) _,_ vec def ( _k_ _j_ )) _≥_ _θ_ def _,_

0 otherwise _,_

(5)

**a** _·_ **b**
where cos( **a** _,_ **b** ) = _∥_ **a** _∥∥_ **b** _∥_ [is the cosine similarity, and] _[ θ]_ [name]
and _θ_ def are predefined similarity thresholds for name and
definition embeddings, respectively.
Using the above mentioned K means inspired algorithm

[33], we partition _K_ into _m_ clusters _C_ 1 _, C_ 2 _, . . ., C_ _m_, where
each _C_ _i_ _⊆_ _K_, [�] _[m]_ _i_ =1 _[C]_ _[i]_ [ =] _[ K]_ [, and] _[ C]_ _[i]_ _[ ∩]_ _[C]_ _[j]_ [ =] _[ ∅]_ [for] _[ i][ ̸]_ [=] _[ j]_ [.]
Each cluster _C_ _i_ represents a candidate class for the ontology,
aggregating key elements that are semantically similar based
on their names and definitions. The clustering process is
performed in batches to handle large datasets efficiently, with
each batch processed independently and then merged with
existing class.
The knowledge graph _C_ = ( _V, E_ ) is constructed as follows:
- _V_ = _{C_ 1 _, C_ 2 _, . . ., C_ _m_ _}_, the set of candidate classes. _E ⊆_ _V × V_, where an edge ( _C_ _i_ _, C_ _j_ ) _∈_ _E_ exists if there
is a relationship between any key element in _C_ _i_ and any key
element in _C_ _j_, with the relationship type _r_ ( _C_ _i_ _, C_ _j_ ) inherited
from the original relationships between local nouns.
Each class _C_ _i_ has a set of properties _P_ ( _C_ _i_ ), computed as
the union of properties of its constituent key elements:

_P_ ( _C_ _i_ ) = � _P_ ( _k_ ) _,_ (6)

_k∈C_ _i_

where _P_ ( _k_ ) includes the name, definition, and other attributes
of the key element _l_ . This knowledge graph serves as the
foundation for ontology creation, enabling structured reasoning over interconnected entities.

_F. Ontology Creation_

The ontology creation stage derives final ontology classes,
relationships, and properties from the knowledge graph _G_,
enabling a structured representation of the unstructured electrical relay documents. We apply leiden community detection
algorithm to partition the graph into communities (ontology
classes), extract inter-community relationships, and synthesize
class properties using Gemini 2.5 Flash. The process is formalized using mathematical notations to describe the graph
structure, partitioning, and relationship projections.
The knowledge graph _C_ = ( _V, E_ ) is a directed graph,
where _V_ = _{C_ 1 _, C_ 2 _, . . ., C_ _m_ _}_ is the set of candidate classes
(clusters of key elements), and _E ⊆_ _V × V_ is the set of
directed edges (relationships). Each node _C_ _i_ _∈_ _V_ has a set
of properties _P_ ( _C_ _i_ ), and each edge ( _C_ _i_ _, C_ _j_ ) _∈_ _E_ is labeled
with a relationship type _r_ ( _C_ _i_ _, C_ _j_ ). The ontology creation
process transforms _C_ into an ontology graph with classes,
relationships, and properties, as detailed below.
_1) Community Detection:_ We apply the Leiden algorithm [30] to partition _V_ into _k_ disjoint communities
_O_ 1 _, O_ 2 _, . . ., O_ _k_, where each _O_ _i_ _⊆_ _V_, [�] _[k]_ _i_ =1 _[O]_ _[i]_ [ =] _[ V]_ [, and]
_O_ _i_ _∩_ _O_ _j_ = _∅_ for _i ̸_ = _j_ . These communities represent ontology classes, grouping candidate classes that share structural
and semantic relationships within the knowledge graph. The
quality of the partitioning is measured using modularity _Q_,
defined as:


_δ_ ( _c_ _i_ _, c_ _j_ ) _,_ (7)
�


4

where _m_ = _|E|_ is the number of edges in _G_, _A_ _ij_ is the
adjacency matrix entry (1 if ( _i, j_ ) _∈_ _E_, 0 otherwise), _k_ _i_
is the degree of node _i_, _c_ _i_ is the community of node _i_,
and _δ_ ( _c_ _i_ _, c_ _j_ ) = 1 if _c_ _i_ = _c_ _j_, 0 otherwise. A human-in-theloop approach guided the selection of _k_ to ensure meaningful
ontology classes for the electrical relay domain.
_2) Class Property Extraction:_ For each community _O_ _i_, we
aggregate the properties of its candidate classes and intracommunity relationships to define class properties. The aggregated property set _P_ ( _O_ _i_ ) is computed as:

_P_ ( _O_ _i_ ) = � _P_ ( _C_ ) _∪{r_ ( _C_ _a_ _, C_ _b_ ) _| C_ _a_ _, C_ _b_ _∈_ _C_ _i_ _,_ ( _C_ _a_ _, C_ _b_ ) _∈_ _E}._

_C∈O_ _i_

(8)
Gemini 2.5 Flash then synthesizes generalized class properties _P_ gen ( _O_ _i_ ) from _P_ ( _O_ _i_ ), handling synonyms and variations
through a function _f_ LLM :

_P_ gen ( _O_ _i_ ) = _f_ LLM ( _P_ ( _O_ _i_ )) _,_ (9)

where _f_ LLM ensures the resulting properties are consistent and
applicable to the entire class.
_3) Relationship Extraction:_ Inter-community relationships
are extracted by projecting edges between classes in different
communities onto the communities themselves. For an edge
( _G_ _a_ _, G_ _b_ ) _∈_ _E_ where _G_ _a_ _∈_ _O_ _i_, _G_ _b_ _∈_ _O_ _j_, and _i ̸_ = _j_, we
define a class-level relationship _R_ ( _O_ _i_ _, O_ _j_ ):

_R_ ( _C_ _i_ _, C_ _j_ ) = _{r_ ( _G_ _a_ _, G_ _b_ ) _| G_ _a_ _∈_ _C_ _i_ _, G_ _b_ _∈_ _C_ _j_ _,_ ( _G_ _a_ _, G_ _b_ ) _∈_ _E}._
(10)
Intra-community relationships (where _G_ _a_ _, G_ _b_ _∈_ _C_ _i_ ) are used
for property extraction and excluded from _R_ . To simplify
the ontology, pairs of directed relationships _r_ ( _O_ _i_ _, O_ _j_ ) and
_r_ ( _O_ _j_ _, O_ _i_ ) with identical labels are merged into a single
undirected relationship.
_4) Sub-Community_ _Detection:_ To create a hierarchical ontology, we perform sub-community detection within
each community _O_ _i_, partitioning it into sub-communities
_O_ _i_ 1 _, O_ _i_ 2 _, . . ., O_ _im_, where [�] _[m]_ _j_ =1 _[O]_ _[ij]_ [ =] _[ O]_ _[i]_ [ and] _[ O]_ _[ij]_ _[ ∩]_ _[O]_ _[il]_ [ =] _[ ∅]_
for _j ̸_ = _l_ . This process mirrors the initial community detection
but operates on subgraphs _G_ [ _O_ _i_ ]. Hierarchical relationships
are defined using set inclusion:

_O_ _ij_ _⊆_ _O_ _i_ (“IS-A” relationship) _,_ (11)

with the inverse _O_ _i_ _⊇_ _O_ _ij_ (“HAS-A” relationship). Properties
and relationships for sub-communities are extracted similarly
to the primary communities, enhancing the ontology’s granularity.

III. R ETRIEVAL M ETHODOLOGY

OntoRAG employs a structured retrieval process to leverage its ontology for answering sensemaking questions over
the electrical relay corpus. The retrieval process begins by
extracting key elements from the user’s query using Gemini
2.5 Flash. These key elements, typically entities or concepts
(e.g., “failure modes” or “relay components”), are treated as
names, while the query itself is considered a definition.
For each key element, a similarity search is performed
against the ontology classes at a specified level (e.g., O1,


1
_Q_ =
2 _m_


�

_i,j_


_A_ _ij_ _−_ _[k]_ _[i]_ _[k]_ _[j]_
� 2 _m_

5


Extract entities,
relationships


Hybrid &



Extract text,














Fig. 3. Detailed workflow of the OntoRAG pipeline, with emphasis on the Ontology Creation stage.


O2), which is a configurable parameter. The similarity search
compares the name of the key element (via its embedding) and
the query’s definition (via its embedding) with the embeddings
of ontology class names and definitions, respectively, using
cosine similarity. The ontology level determines the granularity
of the classes used for retrieval, with higher levels (e.g., O3)
providing finer granularity and lower levels (e.g., O0) offering
more abstraction.

Based on the similarity search results, the top-matching
ontology classes are identified, and their corresponding source
chunks are retrieved from the corpus. To ensure sufficient
context, an expanded context window—also configurable—is
applied around each retrieved chunk (e.g., extending by 200
tokens on either side). These retrieved chunks are then combined to form the final context, which is used by Gemini 2.5
Flash to generate the answer. This ontology-driven retrieval
process enables OntoRAG to capture global relationships
and hierarchical structures, enhancing its ability to address
complex sensemaking questions.

IV. E XPERIMENTAL S ETUP

We evaluated OntoRAG’s effectiveness for global sensemaking tasks, adopting a methodology inspired by GraphRAG

[8]. OntoRAG was compared against GraphRAG and a vector RAG baseline (SS), focusing on high-level sensemaking
questions over a corpus of electrical relay documents.

_A. Dataset_

The evaluation used a single dataset:

_•_ **Electrical Relay Documents** : A proprietary corpus of
PDF documents (technical manuals, datasheets, application notes) for ABB relay products totaling approximately


1 million tokens, divided into 1600 chunks of 600 tokens
each with 100-token overlaps.

The dataset was processed through OntoRAG to generate
knowledge graphs and ontologies, while GraphRAG and SS
used their respective indexing methods.

_B. Conditions_

Nine conditions were evaluated:

_•_ **O0–O3** : OntoRAG at four ontology levels, from
root(candidate classes)-level (O0) to low-level (O3), providing varying granularity.

_•_ **C0–C3** : GraphRAG at four community levels, from rootlevel (C0) to low-level (C3) [8].

_•_ **SS** : Vector RAG baseline using semantic search [8].

_C. Question Generation_

Following [8], we generated 125 sensemaking questions
using Gemini 2.5 Flash:

1) Described the dataset and use cases (e.g., technical
support, design optimization).
2) Created personas for five users (e.g., relay technician)
with five tasks each (e.g., troubleshooting).
3) Generated five questions per user-task pair (e.g., “What
are common failure modes of electrical relays across
manufacturers?”).

Questions were validated by domain experts for relevance.

_D. Evaluation Metrics_

We used head-to-head comparisons with Gemini 2.5 Flash
as the judge, evaluating four metrics [8].:

_•_ **Comprehensiveness** : Coverage of question aspects without redundancy.

_•_ **Diversity** : Variety of perspectives in the answer.

_•_ **Empowerment** : Ability to enable informed judgments.

_•_ **Directness** : Specificity in addressing the question.

Answers were compared pairwise, with five replicates per
question to account for LLM stochasticity. Additionally, claimbased metrics were computed:

_•_ **Comprehensiveness** : Average number of unique claims,
extracted using Claimify [8].

_•_ **Diversity** : Average number of claim clusters, computed using Scikit-learn’s agglomerative clustering with
ROUGE-L distance [32].

_E. Configuration_

OntoRAG utilized the Unstructured library for PDF parsing,
PyMuPDF and Gemini 2.5 Flash for table/image extraction,
entity/relationship extraction, ontology creation, question generation, and evaluation. Community detection used the Leiden
algorithm via graspologic [31]. Indexing took approximately
300 minutes on a virtual machine (16GB RAM, Intel Xeon
Platinum 8171M CPU @ 2.60GHz).

V. R ESULTS AND D ISCUSSION

OntoRAG was evaluated across ontology levels (O0–O3)
against GraphRAG (C0–C3) and SS, focusing on qualitative
and claim-based metrics.

_A. Qualitative Results_

Table I shows win rates for OntoRAG (O2) against other
conditions. O2 achieved comprehensiveness win rates of 88%
against SS (p ¡ 0.001) and 65% against GraphRAG’s best
configuration (C2, p ¡ 0.05), with diversity win rates of
86% and 62%, respectively, demonstrating superior global
sensemaking. Empowerment was slightly higher against SS
(55%, p ¡ 0.05) and comparable to C2 (52%, p ¿ 0.05). SS
excelled in directness (92%, p ¡ 0.001), while O2 outperformed
C2 in directness (58%, p ¡ 0.05).
Figure 4 illustrates the performance trends across ontology and community levels for OntoRAG and GraphRAG.
OntoRAG consistently outperforms GraphRAG in both comprehensiveness and diversity, with O2 achieving the highest

scores.

_B. Claim-Based Results_

Table II presents claim-based metrics. O2 achieved 35.2
claims and 14.1 clusters per answer, slightly surpassing
C2 (34.8 claims, 13.8 clusters). Other OntoRAG levels
ranged from 34.9–35.3 claims and 13.9–14.2 clusters, while
GraphRAG levels ranged from 34.5–34.8 claims and 13.6–13.8
clusters. SS lagged at 26.5 claims and 8.0 clusters.


6

_C. Discussion_

OntoRAG’s ontology-driven approach enhances global
sensemaking in the electrical relay domain, where structured
relationships are critical. The hierarchical ontology levels
enable a balance of granularity and abstraction, with O2
outperforming SS (88% comprehensiveness, 86% diversity)
and C2 (65% comprehensiveness, 62% diversity). The line plot
(Fig. 4) shows OntoRAG’s consistent edge over GraphRAG
across all levels, with a slight but significant improvement in
both comprehensiveness and diversity. Empowerment results
are comparable between O2 and C2 (52%, p ¿ 0.05), indicating
that both methods enable informed judgments similarly. However, SS’s strength in directness (92%, p ¡ 0.001) suggests that
OntoRAG may benefit from incorporating local retrieval strategies for specific queries. The computational cost of ontology
creation (300 minutes) exceeds GraphRAG’s indexing (281
minutes), highlighting the need for optimization to improve
scalability.

VI. L IMITATIONS

While OntoRAG demonstrates significant improvements in
global sensemaking, its current implementation faces scalability challenges that hinder its readiness for production
scenarios. The ontology creation process, which involves clustering local nouns into global nouns and applying community
detection, is computationally intensive, taking 300 minutes to
index a 1-million-token corpus on a modest virtual machine
(16GB RAM, Intel Xeon Platinum 8171M CPU @ 2.60GHz).
This high computational cost makes OntoRAG impractical for
larger corpora or real-time applications, limiting its scalability
in production environments. Additionally, OntoRAG’s reliance
on domain-specific prompts for entity extraction, ontology
creation, and question generation reduces its generalizability
across diverse domains, requiring manual effort to adapt
prompts for new use cases. The retrieval process, while
effective, involves multiple similarity searches across ontology
levels, further adding to the computational overhead. These
limitations highlight the need for optimization as the next
step, including more efficient clustering algorithms, adaptive
prompting strategies, and hybrid indexing techniques to reduce
runtime and resource demands.

VII. C ONCLUSION

OntoRAG represents a significant advancement in automated ontology creation, transforming unstructured electrical
relay documents into a queryable ontology for enhanced
question-answering. By integrating LLMs with graph-based
methods, OntoRAG outperforms vector RAG and GraphRAG
in comprehensiveness and diversity, achieving win rates of
88% and 65% against SS and C2, respectively. However,
scalability challenges, such as the high computational cost of
ontology creation and reliance on domain-specific prompts,
limit its readiness for production scenarios. Future work
will focus on optimizing computational efficiency through
advanced indexing techniques, scaling to larger corpora, and
extending the approach to diverse domains with adaptive
prompting strategies.

7

TABLE I

Q UALITATIVE EVALUATION RESULTS FOR O NTO RAG (O2) AGAINST G RAPH RAG (C0–C3) AND SS ON THE ELECTRICAL RELAY DATASET . W IN RATES
(%) ARE AVERAGED OVER FIVE REPLICATES FOR 125 QUESTIONS, WITH P  - VALUES FROM W ILCOXON SIGNED  - RANK TESTS (H OLM -B ONFERRONI
CORRECTED ).

**Condition** **Comprehensiveness** **Diversity** **Empowerment** **Directness**

C0 70 (p ¡ 0.01) 68 (p ¡ 0.01) 45 (p ¿ 0.05) 40 (p ¡ 0.05)
C1 68 (p ¡ 0.01) 66 (p ¡ 0.01) 47 (p ¿ 0.05) 42 (p ¡ 0.05)
C2 65 (p ¡ 0.05) 62 (p ¡ 0.05) 52 (p ¿ 0.05) 58 (p ¡ 0.05)
C3 67 (p ¡ 0.01) 64 (p ¡ 0.01) 48 (p ¿ 0.05) 45 (p ¡ 0.05)
SS 88 (p ¡ 0.001) 86 (p ¡ 0.001) 55 (p ¡ 0.05) 92 (p ¡ 0.001)

Performance Comparison of OntoRAG and GraphRAG Across Levels

90


80



70

60

50

|Col1|Col2|Col3|ess|Col5|Col6|
|---|---|---|---|---|---|
|||OntoRAG Comprehensiven<br>OntoRAG Diversity|ess|ess|ess|
|||GraphRAG Comprehensiven<br>GraphRAG Diversity|ess|||
|||||||
|||||||

0 1 2 3

Level

Fig. 4. Line plot comparing comprehensiveness and diversity win rates of OntoRAG (O0–O3) and GraphRAG (C0–C3) across their respective levels, averaged
over comparisons with SS.

TABLE II

C LAIM  - BASED EVALUATION RESULTS FOR ALL CONDITIONS ON THE ELECTRICAL RELAY DATASET . B OLD VALUES INDICATE THE HIGHEST SCORE .

**Condition** **Avg. Number of Claims** **Avg. Number of Clusters**

O0 34.9 13.9

O1 35.0 14.0

O2 **35.2** **14.1**

O3 35.3 14.2

C0 34.5 13.6

C1 34.6 13.7

C2 34.8 13.8

C3 34.7 13.7

SS 26.5 8.0

8


R EFERENCES

[1] T. Brown et al., “Language models are few-shot learners,” in _Advances in_
_Neural Information Processing Systems_, vol. 33, 2020, pp. 1877–1901.

[2] R. Bommasani et al., “On the opportunities and risks of foundation
models,” _arXiv preprint arXiv:2108.07258_, 2021.

[3] A. Chowdhery et al., “PaLM: Scaling language modeling with pathways,” _Journal of Machine Learning Research_, vol. 24, no. 240, pp.
1–113, 2023.

[4] N. Kandpal et al., “Large language models struggle to learn long-tail
knowledge,” in _International Conference on Machine Learning_, 2023,
pp. 15,696–15,707.

[5] Y. Zhang et al., “Siren’s song in the AI ocean: A survey on hallucination
in large language models,” _arXiv preprint arXiv:2309.01219_, 2023.

[6] U. Khandelwal et al., “Nearest neighbor machine translation,” in _Inter-_
_national Conference on Learning Representations_, 2021.

[7] H. T. Dang, “DUC 2005: Evaluation of question-focused summarization
systems,” in _Proceedings of the Workshop on Task-Focused Summariza-_
_tion and Question Answering_, 2006, pp. 48–55.

[8] D. Edge et al., “From local to global: A Graph RAG approach to queryfocused summarization,” _arXiv preprint arXiv:2404.16130_, 2024.

[9] A. Konys, “Knowledge repository of ontology learning tools from text,”
_Procedia Computer Science_, vol. 159, pp. 1614–1628, 2019.

[10] O. Bodenreider, “The unified medical language system (UMLS): Integrating biomedical terminology,” _Nucleic Acids Research_, vol. 32, no.
suppl 1, pp. D267–D270, 2004.

[11] G. A. Miller, “WordNet: A lexical database for English,” _Communica-_
_tions of the ACM_, vol. 38, no. 11, pp. 39–41, 1995.

[12] T. Rebele et al., “YAGO: A multilingual knowledge base from
Wikipedia, WordNet, and GeoNames,” in _The Semantic Web–ISWC_
_2016_, 2016, pp. 177–185.

[13] S. L. Weibel and T. Koch, “The Dublin Core Metadata Initiative,” _D-Lib_
_Magazine_, vol. 6, no. 12, 2000.

[14] R. V. Guha et al., “Schema.org: Evolution of structured data on the
web,” _Communications of the ACM_, vol. 59, no. 2, pp. 44–51, 2016.

[15] N. F. Noy and D. L. McGuinness, “Ontology development 101: A guide
to creating your first ontology,” Stanford Knowledge Systems Laboratory
Technical Report KSL-01-05, 2001.

[16] D. Jones, T. Bench-Capon, and P. Visser, “Methodologies for ontology
development,” University of Liverpool, 1998.

[17] K. Opasjumruskit, S. Schindler, and P. M. Schafer, “Towards learning from user feedback for ontology-based information extraction,”
in _Proceedings of the 1st International Workshop on Challenges and_
_Experiences from Data Integration to Knowledge Graphs_, 2019.

[18] A. Chakraborty, “Multi-hop question answering over knowledge graphs
using large language models,” _arXiv preprint arXiv:2404.19234_, 2024.

[19] K. Cheng et al., “Multi-hop question answering under temporal knowledge editing,” _arXiv preprint arXiv:2404.00492_, 2024.

[20] A. Zhu et al., “FanOutQA: A multi-hop, multi-document question
answering benchmark for large language models,” _arXiv preprint_
_arXiv:2402.14116_, 2024.

[21] V. K. Kommineni, B. K¨onig-Ries, and S. Samuel, “From human experts
to machines: An LLM supported approach to ontology and knowledge
graph construction,” _arXiv preprint arXiv:2403.08345_, 2024.

[22] T. Berners-Lee, _Weaving the Web: The Original Design and Ultimate_
_Destiny of the World Wide Web_ . HarperCollins, 2000.

[23] “BeautifulSoup,” [Online]. Available:
https://www.crummy.com/software/BeautifulSoup/.

[24] “Scrapy,” [Online]. Available: https://scrapy.org/.

[25] “Unstructured Library,” [Online]. Available: https://unstructured.io/.

[26] “PDFMiner,” [Online]. Available: https://github.com/pdfminer/pdfminer.six.

[27] “PyMuPDF,” [Online]. Available: https://pymupdf.readthedocs.io/.

[28] “Amazon Textract,” [Online]. Available:
https://aws.amazon.com/textract/.

[29] “Pytesseract,” [Online]. Available: https://github.com/madmaze/pytesseract.

[30] V. A. Traag, L. Waltman, and N. J. van Eck, “From Louvain to Leiden:
Guaranteeing well-connected communities,” _Scientific Reports_, vol. 9,
no. 1, pp. 1–12, 2019.

[31] “graspologic,” [Online]. Available: https://microsoft.github.io/graspologic/.

[32] “Scikit-learn,” [Online]. Available: https://scikit-learn.org/.

[33] Abiodun M. Ikotun, Absalom E. Ezugwu, Laith Abualigah, Belal
Abuhaija, Jia Heming,. https://doi.org/10.1016/j.ins.2022.11.139.

