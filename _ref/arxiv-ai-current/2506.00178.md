## **Tournament of Prompts: Evolving LLM Instructions Through** **Structured Debates and Elo Ratings**


Anirudh Nair [*]

Amazon

Boston, MA, USA

rianina@amazon.com


Adi Banerjee [*]

Amazon

New York, NY, USA

adibaner@amazon.com


Laurent Mombaerts

Amazon

Luxembourg, Luxembourg

lmomb@amazon.com


Matthew Hagen

Amazon

Atlanta, GA, USA

mathage@amazon.com

**Abstract**

Prompt engineering represents a critical bottleneck to harness the
full potential of Large Language Models (LLMs) for solving complex
tasks, as it requires specialized expertise, significant trial-and-error,
and manual intervention. This challenge is particularly pronounced
for tasks involving subjective quality assessment, where defining
explicit optimization objectives becomes fundamentally problematic. Existing automated prompt optimization methods falter in
these scenarios, as they typically require well-defined task-specific
numerical fitness functions or rely on generic templates that cannot capture the nuanced requirements of complex use cases. We
introduce DEEVO (DEbate-driven EVOlutionary prompt optimization), a novel framework that guides prompt evolution through a
debate-driven evaluation with an Elo-based selection. Contrary to
prior work, DEEVO‚Äôs approach enables exploration of the discrete
prompt space while preserving semantic coherence through _intelli-_
_gent crossover_ and _strategic mutation_ operations that incorporate
_debate-based feedback_, combining elements from both successful
and unsuccessful prompts based on identified strengths rather than
arbitrary splicing. Using Elo ratings as a fitness proxy, DEEVO simultaneously drives improvement and preserves valuable diversity
in the prompt population. Experimental results demonstrate that
DEEVO significantly outperforms both manual prompt engineering
and alternative state-of-the-art optimization approaches on openended tasks and close-ended tasks despite using no ground truth
feedback. By connecting LLMs‚Äô reasoning capabilities with adaptive optimization, DEEVO represents a significant advancement in

- These authors contributed equally to this work.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
_Prompt Optimization KDD 2025,_
¬© 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-x-xxxx-xxxx-x/YYYY/MM
[https://doi.org/XXXXXXX.XXXXXXX](https://doi.org/XXXXXXX.XXXXXXX)


Tarik Borogovac

Amazon

Boston, MA, USA

tarikbo@amazon.com

prompt optimization research by eliminating the need of predetermined metrics to continuously improve AI systems.

**CCS Concepts**

- **Computing methodologies** ‚Üí **Artificial intelligence** ; **Artifi-**
**cial intelligence** ; **Artificial intelligence** .

**Keywords**

Large Language Models, Multi-Agent Systems, Prompt Optimization, Multi-Agent Debates, Evolutionary Algorithms

**ACM Reference Format:**

Anirudh Nair [*], Adi Banerjee [*], Laurent Mombaerts, Matthew Hagen, and Tarik
Borogovac. 2018. Tournament of Prompts: Evolving LLM Instructions Through
Structured Debates and Elo Ratings. In _Proceedings of August 4‚Äì5, 2024_
_(Prompt Optimization KDD 2025)._ ACM, New York, NY, USA, 15 pages.
[https://doi.org/XXXXXXX.XXXXXXX](https://doi.org/XXXXXXX.XXXXXXX)

**1** **Introduction**

Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse domains, such as literary and professional
writing, code generation, and problems requiring logical reasoning.
However, their performance towards a specific task remains heavily
dependent on the quality of instructions ‚Äî or prompts ‚Äî provided
to them [ 16, 18 ]. The term _prompt engineering_ has become widely
used, signaling that prompting has become an important skill for
harnessing these models‚Äô full potential. This skill requires specialized expertise attained through learning techniques and significant
trial-and-error. Furthermore, when prompts prove insufficient for a
task, developers must either implement dedicated post-processing
logic or employ fine-tuning strategies to address performance gaps.
In that sense, the development of systems executing complex tasks
while solely relying on prompt engineering is resource intensive,
thus motivating the need for an automated method to optimize
prompts.

The automated optimization of prompts is especially challenging for
tasks where performance or quality is judged subjectively, where

Prompt Optimization KDD 2025, Toronto, Canada,
Nair and Banerjee et al.


ambiguity challenges require resolution, or where managing conflicting contexts is paramount [ 50 ]. In these scenarios, agents are expected to learn to adapt synthesizing different types of information,
make judgment calls from different perspectives, and self-ascertain
branching and stopping conditions during its iteration process;
all in the absence of any quality criteria or scoring functions to
quantify success along said criteria.

Current approaches to automated discrete prompt optimization
fall into two primary categories: gradient-based methods and evolutionary strategies. Gradient-based methods operate on textual
gradients defined by means of LLM generated critical feedback (examples of these are Protegi [ 38 ] and TextGrad [ 58 ]). These methods
offer computational efficiency but typically require labeled ground
truth data from which to calculate loss; risk task-specific overfitting
especially when there is a lack of diversity in the examples; and
do not have a mechanism to perform exploration and thus suffer
from adaptability issues. Conversely, evolutionary methods (such
as EvoPrompt [ 18 ] and PromptBreeder [ 16 ]) provide broader exploration capabilities but suffer from computational inefficiency
due to random search and, crucially, depend on well-defined objective fitness functions ‚Äî which are often unavailable for subjective
tasks.

We introduce DEEVO (DEbate-driven EVOlutionary prompt optimization), a novel framework that addresses the challenges of 1)
exploration vs exploitation, and 2) lack of labeled ground truth /
fitness functions, by guiding prompt evolution through structured
debates and Elo-based selection.

**Exploration vs Exploitation**
Unlike previous approaches, DEEVO enables systematic exploration
of the prompt space through two innovative evolutionary mechanisms. At its core, DEEVO employs multi-agent debate [ 13 ] to guide
intelligent crossover, where strengths and weaknesses of parent
prompts are identified before strategically combining their effective elements. This process is complemented by targeted prompt
mutations that specifically modify instructions to improve task
performance. DEEVO‚Äôs evolutionary approach selectively incorporates elements from both successful and unsuccessful prompts
based on their identified strengths, preserving prompt effectiveness
and logical structure while systematically exploring the solution

space.

**Lack of Labeled Ground-Truth Data**

Unlike prior prompt optimization strategies, DEEVO does not rely
on labeled data / fitness functions against ground truth in order to
evaluate prompt effectiveness. Instead, it leverages LLM-powered
multi-agent debates (MAD) [ 13 ] to evaluate prompt quality without
requiring predetermined metrics. By having LLMs critique prompt
outputs in a pairwise fashion and determine a winner through
structured debates, DEEVO ensures a self-contained evaluation system that can assess quality across diverse tasks ‚Äî including those
with subjective criteria. The evaluation mechanism evolves and
improves along with the prompts, incorporating novel ideas as candidate criteria for future evaluation. The resulting debate verdicts
serve as a fitness proxy that simultaneously drives improvement
and preserves valuable diversity in the prompt population without


needing a manually crafted or separately learned objective function
for fitness selection.

**A Modified Elo-Based Selection**
The Elo rating system [ 14 ] is a robust method to rank entities (in
this case, prompts) via pairwise comparisons, where each prompt
maintains a numerical rating that dynamically updates based on
competition outcomes. This approach has gained significant traction in LLM evaluation frameworks, with numerous benchmark
systems adopting Elo-based mechanisms to rank model and prompt
performance [ 2, 4, 10 ]. Despite its robustness, Elo is particularly limited in its ability to handle newcomer prompt competitors (due to requiring many matchups to reach an accurate skill assessment); and
veteran prompt competitors (due to their ratings becoming "sticky"
over time, as historical matchups dilute recent performances). For
this reason, DEEVO utilizes a modified Elo-selection mechanism
that introduces selection quotas for newcomers and veterans; to
force the prompt population to always consist of a balanced proportion of newcomer and veteran prompts. This achieves faster
calibration for new prompt candidates, better capture current skill
levels for veterans, and provide more accurate performance esti
mates.

We demonstrate that DEEVO significantly outperforms both manual prompt engineering and alternative optimization approaches
across both open and close ended tasks. By connecting LLMs‚Äô reasoning capabilities with adaptive optimization, DEEVO eliminates
the need for developing predetermined metrics to continuously
optimize prompts, opening new possibilities for self-improving AI
systems across domains where subjective quality assessment is
essential.

**2** **Related Works**

**2.1** **Prompt Optimization**

Prompt optimization has emerged as a critical area of research in
large language model (LLM) development, with researchers exploring various techniques to enhance model performance through
systematic refinement of input prompts. Historically, researchers
have relied on manual supervised approaches, using black-box techniques that score prompts based on observable output metrics such
as accuracy, F1 score, BLEU, or ROUGE. However more recently, automatic prompt optimization has come into light as an alternative,
scalable solution [ 43 ]. Soft prompt optimization operates in a continuous space and usually involves some gradient-based operator.
These methods operate in a continuous space range by leveraging
embeddings to automatically optimize the prompts [ 23, 31, 49, 60 ],
training auxiliary models to output optimized prompts [ 9, 11, 24,
48, 61 ], or using non-gradient approaches to adjust prompt representations [8].

In spite of effective performance, continuous methods often lack interpretability [ 30 ], require model training [ 65 ], or need access to at
least partial knowledge of the internals of an LLM [ 37 ], something
out of scope for black-box LLM APIs. Contrary to optimizing in a
continuous space, discrete prompt optimization methods work in
a non-differentiable space, treating prompts as fixed textual structures and refining them directly [ 64 ]. While discrete methods do

Prompt Optimization KDD 2025, Toronto, Canada,
Tournament of Prompts: Evolving LLM Instructions Through Structured Debates and Elo Ratings

**Figure 1: DEEVO. (1) First an initial set of prompts are either provided or generated. (2) Next, each prompt is executed. (3) Each**
**output is then is then paired with another and then passed into a Multi-Agent Debate evaluation to determine a winner. (4) A**
**Crossover agent then leverages the debate trace to intelligently create a new prompt that combines the strong elements of**
**both prompts in each pair. (5) Some child prompts are then randomly selected to go through a task-driven Mutation agent. (6)**
**Finally, the Elo ratings are updated based on the winner and loser prompts while children are given a base rating of 1000, and**
**the next generation repeats.**


not involve gradient operations for prompt optimization, several
methods have developed gradient-like mechanisms (aptly named
‚Äôtextual gradients‚Äô) [ 28, 39, 58 ] that mimic their numerical counterparts. ProTeGi [ 39 ] employs an LLM-feedback system to generate
gradients in the form of natural language text that compares the
output of the executed prompt and the ground-truth result and then
uses beam search to iteratively refine the prompt. Textual gradientbased methods offer computational efficiency through reduced LLM
calls but depend on ground truth data. To address this limitation,
methods like PACE [ 12 ] and SPO [ 54 ] leverage the LLM itself for
output evaluation. While PACE requires a scoring function, SPO utilizes pairwise comparison to select superior prompts. Another line
of work in discrete prompt optimization is leveraging evolutionary
strategies. EvoPrompt [ 18 ] uses genetic algorithms and differential evolution [ 45 ] to crossover and mutate different prompts in a
population. PromptBreeder [ 16 ] samples different thinking styles
and mutation operators to generate the population and then run a
binary tournament genetic algorithm [ 19 ]. Survival-of-the-Safest

[ 44 ] interleaves different objectives for multi-task secure prompt
optimization through exhaustive and sequential evolutionary strategies. A benefit of evolutionary prompt optimization methods is that
they do not rely on ground truth examples, unlike textual gradients;
however, such methods do require task-based fitness functions for
scoring. Such functions are manually-intensive to craft and may
be intractable for very complex tasks. To mitigate this, DEEVO
integrates structured multi-agent debates with an Elo rating system
to guide evolutionary prompt optimization without requiring either
manually-crafted metrics or ground-truth labels.


**2.2** **Multi-Agent Debate**

There is extensive research around multi-agent debate (MAD) utilizing LLMs [ 35, 36, 40 ]. In the realm of autonomous pairwise comparison, ChatEval [ 6 ] and Debatrix [ 32 ] have debaters take turns
arguing over which output is better before a final LLM-judge takes
the arguments and makes a decision. In fact, it has been shown
that having more persuasive debaters results in more truthful answers and comparisons [ 35 ] compared to single-pass LLM-judges.
MAD has also been used for numerical scoring: DEBATE [ 29 ] uses
a ‚Äôscorer‚Äô agent that scores an output based on some criteria while
a ‚Äôdevil‚Äôs advocate‚Äô agent debates against the score as much as
possible. Beyond evaluation, MAD has been used for improving
factuality in LLM generation [ 13, 33 ] as well as improving human
learning for writing reports [ 25 ]. Furthermore, multi-agent debate
has been used in optimization of LLMs [ 15, 47 ] and agentic workflows [46].

Recent developments build on the premise that prompts can be optimized through competition or discourse. ZeroSumEval [ 1 ] extends
this by evaluating both prompts and models in zero-sum games.
These frameworks establish dynamic ecosystems of prompts that
evolve over time, enabling more robust exploration and discovery. Hybrid systems like PromptBoosting [ 22 ], PREFER [ 59 ], and
PromptWizard [ 21 ] enable verifier-editor roles that refine prompts
iteratively.

Building on these advances in multi-agent debate, DEEVO integrates MAD as a core component of its evolutionary process. Specifically, MAD serves as a fitness function to evaluate prompt quality,

Prompt Optimization KDD 2025, Toronto, Canada,
Nair and Banerjee et al.


which then guides the selection of prompts for subsequent generations. This approach not only enables more nuanced evaluation
of individual prompts but opens possibilities for evaluating entire
prompt orchestrations in multi-agent systems, where the collective performance of agent prompts can be jointly assessed through
structured debates.

**2.3** **Elo Ratings for LLMs**

The adoption of the Elo rating system [ 14 ] to evaluate LLMs represents a significant methodological advancement in AI benchmarking, enabling relative performance assessments through pairwise
comparisons rather than absolute scoring methods. In benchmarking, Elo has been used in Chatbot Arena [ 10 ] and WildBench [ 34 ]
to rank chatbot performance through crowdsourced tasks and pairwise comparisons. Beyond leaderboards, the theoretical analysis of
Elo as a metric for LLM ranking and evaluation has been heavily
studied recently. In Chatbot Arena, Elo has been shown to outperform more complex algorithms, like mElo [ 3 ] and Bradley-Terry

[ 5 ], as well as pairwise comparison algorithms, like winrates, as
a more robust evaluation rating [ 51 ]. Moreover, the robustness of
Elo with respect to fundamental evaluation properties like transitivity and reliability, increase with the number of permutations

[ 4 ]. While the original Elo system does not incorporate ties, it has
been extended to consider ties for LLM ranking [ 2 ] using the Rao
& Kupper method [41].

While Elo has been more typically studied as an evaluation and
ranking system for LLMs and machine learning models in general,
its numerical properties has led to them being used for optimization
of such models as well. In Elo-Rating Based Reinforcement Learning
(ERRL) [ 26 ], the Elo system is used to rank human trajectories and
convert ordinal rewards into cardinal rewards for preference-based
reinforcement learning (RL). Reward Reasoning Models (RRMs)

[ 17 ] use Elo and a knockout tournament structure as a rewarding strategy to train an LLM via RL. REvolve [ 20 ] leverages an
evolutionary algorithm to evolve the reward function for RL and
convert pairwise human-feedback into a fitness score using Elo.
Inspired by these methods, DEEVO utilizes Elo as a fitness function to guide the evolutionary prompt optimization, bypassing the
need for a manually crafted or learned fitness function or reward
model.

**3** **DEEVO**

DEEVO (Debate-Driven Evolutionary Prompt Optimization) is a
novel prompt optimization framework that combines evolutionary algorithms with multi-agent debate evaluation to efficiently
discover high-quality prompts for large language models. Unlike traditional evolutionary algorithms that rely on fixed fitness functions,
DEEVO leverages the emergent capabilities of language models
themselves through structured debates to evaluate prompt quality.
A diagram of the DEEVO workflow is shown in Figure 1.

Assume we have access to a set of tasks T = { _ùë°_ 0 _,ùë°_ 1 _, . . .,ùë°_ _ùëõ_ }, and a
set of initial prompts P = { _ùëù_ 0 _, ùëù_ 1 _, . . ., ùëù_ _ùëÄ_ } . We also assume there
is access to an LLM via a black-box API.


**Algorithm 1** DEEVO: Debate-Driven Evolutionary Prompt Opti
mization

**Require:** Tasks T, initial prompts P, population size _ùëõ_, generations _ùê∫_, mutation rate _ùëö_, newcomer quota _ùëõ_ _ùëõùëíùë§_, _ùëë_ debate rounds

Initialize population with prompts and set Elo ratings to 1000
**for** gen = 1 to _ùê∫_ **do**

Form random prompt pairs from population
_// in parallel_
**for** each pair ( _ùëù_ _ùëé_ _, ùëù_ _ùëè_ ) **do**

Sample task _ùë°_ ‚ààT and input _ùë•_ _ùë°_
Generate responses _ùëü_ _ùëé_ _,ùëü_ _ùëè_ using _ùëù_ _ùëé_ _, ùëù_ _ùëè_ on _ùë•_ _ùë°_
Conduct _ùëë_ -round debate to evaluate responses for task _ùë°_
Determine winner _ùë§_ ‚àà{ _ùëù_ _ùëé_ _, ùëù_ _ùëè_ } and update Elo (Alg. 2)
Create offspring via _Intelligent Crossover_
**if** _ùëüùëéùëõùëëùëúùëö_ () _< ùëö_ **then**

Apply _Strategic Mutation_
**end if**

Add offspring to pool
**end for**

Age all existing prompts
Select next generation:

    - Select newcomers from offspring by _ùëõ_ _ùëõùëíùë§_

    - Select remaining _ùëõ_ ‚àí _ùëõ_ _ùëõùëíùë§_ veterans by Elo
Save best prompts from current generation
**end for**

**return** Top prompt by Elo rating

**3.1** **Framework**

**Step 1: Initialization** To conduct DEEVO, we begin by assigning
each prompt in the initial population with a base Elo rating of 1000
and an age of 0. Each prompt is then paired randomly with another
prompt to form evaluation pairs. If the provided initial prompt set
P is insufficient to create a population of the desired size, DEEVO
generates additional prompts through simple variations of existing
ones. Each prompt is assigned a unique identifier for tracking its
performance and age throughout the evolutionary process. We also
initialize a mutation rate _ùëö_ ‚àà[ 0 _,_ 1 ], the newcomer quota _ùëõ_ _ùëõùëíùë§_ and
debate rounds _ùëë_ .

**Step 2: Evaluation** For each prompt pair ( _ùëù_ _ùëñ_ _, ùëù_ _ùëó_ ), DEEVO conducts a multi-agent debate to determine the superior prompt. First,
both prompts are used with the same LLM to generate responses
to a randomly selected test input _ùë°_ from the task domain T . These
responses, denoted as _ùëü_ _ùëñ_ and _ùëü_ _ùëó_, are then evaluated through a structured debate process:

 - A debate manager prompts the LLM to analyze both responses
in the context of the given task

 - The LLM engages in a multi-round debate, critically evaluating
the strengths and weaknesses of each response

 - In each round, the debate builds upon previous arguments,
allowing for deeper analysis

 - After _ùëë_ rounds, the LLM renders a final verdict declaring either
response _ùëü_ _ùëñ_ or _ùëü_ _ùëó_ as superior

Prompt Optimization KDD 2025, Toronto, Canada,
Tournament of Prompts: Evolving LLM Instructions Through Structured Debates and Elo Ratings


This debate-based evaluation creates a dynamic fitness function
that leverages the LLM‚Äôs own reasoning capabilities rather than
relying on static metrics or human evaluation. The transcript of the
debate provides valuable insights into why certain prompts perform
better, informing the subsequent evolutionary processes.

**Step 3: Crossover & Mutation** DEEVO employs debate-informed
genetic operations to evolve the prompt population. These operations, named _Intelligent Crossover_ and _Strategic Mutation_, leverage
the debate information from the previous step to guide the evolutionary process. After each debate determines a winner between
prompts _ùëù_ _ùëñ_ and _ùëù_ _ùëó_, rather than simple text mixing, DEEVO performs _Intelligent Crossover_ with an LLM that considers the debate
transcript to identify effective components of each prompt for the
task. The winning prompt contributes more genetic material, while
valuable elements from the losing prompt may still be incorporated based on debate insights. Afterwards, using mutation rate _ùëö_,
some offspring are put through a _Strategic Mutation_ process. In this
process, an LLM is asked to either

 - Add a new instruction that enhances its effectiveness or addresses a gap

 - Modify an existing instruction to make it clearer, more precise,
or more effective

 - Remove redundant, ineffective, or potentially harmful parts

 - Restructure the prompt to improve flow, coherence, or clarity

These genetically informed operations result in offspring prompts
that inherit beneficial characteristics while addressing limitations
identified through debate.

**Algorithm 2** Update Elo

**Require:** prompts _ùëù_ _ùëñ_, _ùëù_ _ùëó_, winner, K

_ùëü_ _ùëñ_ ‚Üê Elo rating of prompt _ùëù_ _ùëñ_
_ùëü_ _ùëó_ ‚Üê Elo rating of prompt _ùëù_ _ùëó_
1
_ùëí_ _ùëñ_ ‚Üê
1+10 [(] _[ùëüùëó]_ [‚àí] _[ùëüùëñ]_ [)/][400]
1
_ùëí_ _ùëó_ ‚Üê 1+10 [(] _[ùëüùëñ]_ [‚àí] _[ùëüùëó]_ [)/][400]
_ùë†_ _ùëñ_ ‚Üê 1 if winner = _ùëù_ _ùëñ_, 0 otherwise
_ùë†_ _ùëó_ ‚Üê 1 if winner = _ùëù_ _ùëó_, 0 otherwise
_ùëü_ _ùëñ_ ‚Üê _ùëü_ _ùëñ_ + _ùêæ_ ( _ùë†_ _ùëñ_ ‚àí _ùëí_ _ùëñ_ )
_ùëü_ _ùëó_ ‚Üê _ùëü_ _ùëó_ + _ùêæ_ ( _ùë†_ _ùëó_ ‚àí _ùëí_ _ùëó_ )
**return** Updated ratings _ùëü_ _ùëñ_ _,ùëü_ _ùëó_

**Step 4: Elo Update & Selection** After each debate and offspring
generation, DEEVO updates the population using an Elo-based
selection mechanism:

 - _Elo Rating Update_ : For each prompt pair ( _ùëù_ _ùëñ_ _, ùëù_ _ùëó_ ) with a determined winner, Elo ratings are updated according to Algorithm
2. This process calculates the expected scores _ùëí_ _ùëñ_ and _ùëí_ _ùëó_ based on
current ratings, then adjusts each prompt‚Äôs rating based on the
difference between actual and expected outcomes. The update
formula:

_ùëü_ _ùëñ_ [‚Ä≤] [=] _[ ùëü]_ _[ùëñ]_ [+] _[ ùêæ]_ [¬∑ (] _[ùë†]_ _[ùëñ]_ [‚àí] _[ùëí]_ _[ùëñ]_ [)] (1)


where _ùëü_ _ùëñ_ is the current rating, _ùëí_ _ùëñ_ is the expected score calculated
1
as
1+10 [(] _[ùëüùëó]_ [‚àí] _[ùëüùëñ]_ [)/][400] [,] _[ ùë†]_ _[ùëñ]_ [is the actual score (1 for win, 0 for loss), and]
_ùêæ_ is a constant determining rating volatility.

 - _Age Increment_ : All existing prompts have their age incremented
by 1, tracking their longevity in the population.

 - _Population Selection_ : The next generation‚Äôs population is selected using three distinct pools:

**‚Äì** _Newcomers_ : Top Elo-rated offspring prompts (with age 0),
comprising _ùëõ_ _ùëõùëíùë§_ of the population

**‚Äì** _General Selection_ : Remaining _ùëõ_ ‚àí _ùëõ_ _ùëõùëíùë§_ spots filled by the
highest Elo-rated prompts regardless of age

This selection strategy maintains a balance between exploitation
(keeping high-performing prompts) and exploration (introducing
new variations). Combined with the Elo rating system that reflects
relative performance history, this approach creates a robust evolutionary process that consistently improves prompt quality over
successive generations.

By using the multi-agent debate for evaluation and Elo ratings
as a generic proxy for the fitness function for selection, DEEVO
bypasses the need for ground truth examples and a manually crafted
objective fitness function. We also present the details of DEEVO in
Algorithm 1.

**4** **Experiments**

In this section, we evaluate DEEVO across multiple prompt engineering tasks to demonstrate its effectiveness in optimizing prompts
for various applications. We assess DEEVO‚Äôs ability to discover highquality prompts that enhance LLM performance on reasoning tasks,
instruction following, and creative generation without requiring
human evaluation or labeled data.

**4.1** **Setup**

**Datasets** We adopt DEEVO an datasets that cover both _close-ended_,
where there is ground truth available, and _open-ended_ tasks, where
ground truth outputs are unavailable. For close-ended tasks, we
utilize two datasets:

 - **ABCD** [ 7 ] is a dataset to study dialogue systems in realistic settings - more specifically, customer service in a retail (clothing)
context. Here, an agent‚Äôs actions must be balanced between the
desires expressed by the customer and the constraints set for
what a customer service representative can/not do.

 - **BBH-Navigate** (BBH-Nav) [ 50 ] is a dataset in which given a
series of navigation steps to an agent, determine whether the
agent would end up back at its initial starting point. For testing,
we sampled portions from original datasets as test sets [55].

For open-ended tasks, we use:

 - **MT-Bench** [ 62 ], where we choose three categories of tasks:
_writing_, _roleplay_, and _humanities_ . Each category has 10 subtasks;
we sample 5 subtasks for training and the remaining 5 to test.

Prompt Optimization KDD 2025, Toronto, Canada,
Nair and Banerjee et al.

**Table 1: Comparison of performance between conventional**
**prompt methods and prompt optimization methods on close-**
**ended benchmarks. All methods are executed with Claude**

**3.5 Sonnet V2 on the test set, with results averaged over three**
**runs. The best performing methods are bolded and second-**
**best are underlined.**


|Method|Dataset<br>BBH-Nav p > | t | ABCD p > | t ||
|---|---|
|Direct<br>CoT [53]<br>BRIDGE [52]<br>PromptBreeder [16]<br>SPO [54]|91.3 <0.01 68.5% <0.01<br>89.7 <0.01 74.5% <0.05<br>84.3 <0.01 68.6% <0.01<br>96.3 <0.01 49.1% <0.01<br>97.2 <0.05 77.3% <0.05|
|DEEVO (ours)|97.0 <0.05 83.7% <0.05|


**Baselines** We compare DEEVO to four main methods: Chain-ofThought (CoT) [ 53 ], BRIDGE [ 52 ], PromptBreeder [ 16 ], and SelfSupervised Prompt Optimization (SPO) [ 54 ]. Additionally, we also
benchmark the direct invocation of the LLM (which we call "Direct")
on each task. We implement Direct, CoT, PromptBreeder, SPO and
BRIDGE on ABCD and BBH-Navigate, and we adopt MT-Bench for
comparison with SPO and Direct.

**Metrics** We evaluate performance using accuracy for the ABCD
benchmark and F1-score for BBH-Navigate on the held-out test
sets. For the open-ended MT-Bench, we use winrates as the metric
for evaluation based on the LLM-judge prompt from the original
paper for pairwise comparison. Since LLM-judges for pairwise
comparative evaluation suffer from positional bias [ 63 ] and length
bias [ 56 ], we run 20 independent samples of randomly selected
subtasks from the MT-Bench test set with DEEVO as output A in
the LLM-judge prompt for 10 of the samples and as output B in the
remaining 10.

**Implementation** For CoT, PromptBreeder, BRIDGE, and SPO,
we use the official GitHub implementation for each method. For
DEEVO, we choose 10 initial randomly generated prompts as the
starting population. Across all 3 datasets, we run the evolutionary
process for 5 generations and employ a mutation rate of 0.4. For
the multi-agent debate evaluation, we choose three rounds of debate and one LLM call afterwards to determine the winning output
and, consequently, winning prompt. We use the Claude-3.5-SonnetV2-20241022 model with a temperature of 0.8 for the _Intelligent_
_Crossover_, and _Strategic Mutation_ modules for DEEVO. We also use
temperature 0.8 for the two debating agents in the multi-agent debate module, but a temperature of 0 for the LLM-judge that makes
the final judgment as per prior work [ 27 ]. We use a maximum output token size of 4096. We use Claude-3.5-Sonnet-V2 for all the
other methods, and to maintain consistency, we use a temperature 0 for both training execution and test time execution for all
methods.

**4.2** **Results**

**Close-Ended Tasks** As shown in Table 1, prompts optimized with
DEEVO outperform more established prompting methods (such


**Figure 2: Graph of Elo vs F1-Score for BBH-Nav across 5**
**generations. The Max Elo corresponds to the Elo of the top**
**prompt in the generation and F1-Score is calculated on the**
**test-set for said prompt.**

as direct LLM call, CoT and BRIDGE prompting) as well as other
prompt optimization methods (PromptBreeder and SPO). In both
datasets, we see statistical significance when comparing the performance difference of DEEVO to all other methods. On BBH-Nav,
DEEVO and SPO perform nearly identically, as shown by the similar
F1-scores, and better than the other methods. While neither DEEVO
nor SPO utilized any ground-truth information in their optimization processes on ABCD, SPO struggles compared to DEEVO to
handle the large and complex task of the ABCD dataset, resulting in
DEEVO outperforming SPO by **6.4%** . This is likely because batched
‚Äôtextual-gradient‚Äô methods like SPO suffer from longer context for
the evaluation model as the batch size increases. This is highlighted
in the difference in performance between DEEVO and SPO: BBHNav tasks are small and often a couple of sentences each compared
to the large conversational examples in the ABCD dataset. On the
contrary, while DEEVO does have longer contexts especially from
the multi-agent debate evaluation, it is more robust compared to
the single-pass evaluation in SPO, as seen in prior work [ 6 ]. Despite
not using a ground-truth fitness function, DEEVO also outperforms
fellow evolutionary method PromptBreeder by **0.7** and **34.6%** on
BBH-Nav and ABCD, respectively. This highlights the ability for
Elo to serve as a reliable proxy for a ground truth fitness function
provided enough generations.

Furthermore, Figures 3 and 4 show how Elo scores evolve over
multiple generations (to understand if there is a correlation between its ratings and the accuracies reported in Table 1). Generally,
both average Elo (which are representative of the overall prompt
population at every generation step) and maximum Elo ratings
(which are indicative of the most optimal prompts at every generation step) are trending upwards over generations. Furthermore, our
analysis reveals statistically significant point-biserial correlations
(p < 0.05) between the final prompts‚Äô prediction accuracies and
their Elo ratings, with correlation coefficients of 0.137 for average
Elo and 0.156 for maximum Elo. These correlations demonstrate

Prompt Optimization KDD 2025, Toronto, Canada,
Tournament of Prompts: Evolving LLM Instructions Through Structured Debates and Elo Ratings

**Table 2: Ablation study of DEEVO w.r.t. LLM chosen (Haiku3.5**
**and Llama3-70B) and evaluation style (Single-Pass LLM**
**Judge), on ABCD accuracy results**

|Model Ablation|Eval Style Ablation|Performance|
|---|---|---|
|Haiku-3.5<br>Llama3-70B<br>Sonnet-3.5-V2|Multi-Agent Debate<br>Multi-Agent Debate<br>Single-Pass LLM Judge|76.5%<br>78.6%<br>74.1%|
|Sonnet-3.5-V2|Multi-Agent Debate|83.7%|


a meaningful statistical relationship between Elo ratings and predictive performance. In addition, Figure 2 depicts the relationship
between Elo and the F1-score for the prompt with the highest Elo
on the held out test-set on BBH-Nav. We see that as the Elo in
creases across the generations, the F1-score for the top performing
prompt also increases, showing the correlation between Elo and
task performance.

Finally, an ablation study was performed to examine DEEVO‚Äôs
performance sensitivity to (1) the choice of LLM (by testing ClaudeHaiku-3.5 and Llama3-70B for all aspects of optimization process
i.e. crossover, mutation and debate), and (2) the evaluation strategy
(by using a single-pass LLM judge as a fitness function to compare prompts in a pairwise fashion). As shown in Table 2, DEEVO
performance decreases by **5.1%** when switching from Claude-3.5
‚àº
Sonnet-V2 ( 175B parameters) to Llama3-70B (70B parameters), and
by an additional **2.1%** with Claude-Haiku-3.5 ( ‚àº 20B parameters).
This demonstrates DEEVO‚Äôs scaling potential with increased model
capability. Moreover, switching from multi-agent debate to a less
robust single-pass LLM judge reduces performance by **9.6%**, highlighting the importance of a bias-resistant evaluation mechanism
in DEEVO‚Äôs effectiveness. Notably, smaller models such as ClaudeHaiku-3.5 using DEEVO outperform other prompt optimization
methods that leverage more powerful models.

**Figure 3: Average Elo for DEEVO updates over 5 generations**
**in ABCD. The average Elo increases over time, showing im-**
**provements in the prompt population over the generations.**


**Figure 4: The figure illustrates how the maximum Elo for**
**DEEVO updates over 5 generations in the ABCD use-case.**
**Generally, there is an increasing trend in the maximum Elos**
**(implying improvements in the optimal prompts) over time.**

**Open-Ended Tasks** For MT-Bench, we show the win-rates of
DEEVO over SPO on the three categories: _writing_, _roleplay_, and _hu-_
_manities_ . To show the generalizability and performance of DEEVO
on different LLMs on open-ended tasks, we run an ablation study
comparing DEEVO and SPO on three different LLMs for the entirety
of the execution and optimization (i.e. crossover, mutation and debate): Claude-Sonnet-3.5-V2, Claude-Haiku-3.5, and Llama3-70B.
As described in Section 4.1, we run 20 trials for each category with
10 having DEEVO output as output A and SPO as output B and the
other 10 trials with the outputs in switched positions. We also use
the same LLM-judge prompt from the original MT-Bench paper

[ 62 ], and each model/category combo was run across 3 independent
runs. As shown in Table 3, DEEVO outperforms SPO outputs across
all models for all 3 tasks based on the LLM-judge, regardless of LLM
choice.

To understand the importance of the multi-agent debate component in our approach, we conducted an ablation study comparing
DEEVO against a variant without the debate evaluation (using a
single-pass LLM judge instead). Following the same experimental
protocol as our previous comparison, we evaluated both versions
across the same three categories of MT-Bench ( _writing_, _roleplay_,
and _humanities_ ) using Claude-Sonnet-3.5-V2, Claude-Haiku-3.5,
and Llama3-70B for the entirety of execution and optimization. As
shown in Table 4, DEEVO with the multi-agent debate evaluation
substantially outperforms its ablated variant across all models and
tasks. The win rates are particularly pronounced for the _roleplay_
category (88.3-93.3%), but remain strong across _writing_ (85-95%)
and _humanities_ (80-86.7%) as well. These results demonstrate that
the multi-agent debate component is a critical factor in DEEVO‚Äôs
effectiveness regardless of model choice, providing significant performance benefits compared to using a single-pass evaluation approach.

Prompt Optimization KDD 2025, Toronto, Canada,
Nair and Banerjee et al.


**Table 3: Average win rates of DEEVO over SPO executed on**
**three different models on three different categories of MT-**
**Bench.**

|Model|MT-Bench Categories<br>Writing Roleplay Humanities|
|---|---|
|Sonnet-3.5-V2<br>Haiku-3.5<br>Llama3-70B|81.7% 76% 81.7%<br>85% 75% 66.7%<br>81.7% 71.6% 73.3%|



**Table 4: Average win rates on MT-Bench of DEEVO over**
**DEEVO w/o the debate evaluation on three different models.**

|Model|MT-Bench Categories<br>Writing Roleplay Humanities|
|---|---|
|Sonnet-3.5-V2<br>Haiku-3.5<br>Llama3-70B|88.3% 91.7% 81.7%<br>85% 93.3% 83.3%<br>95% 88.3% 86.7%|



**5** **Limitations**

Despite the promising results exhibited by DEEVO over other
prompting and optimization methods, there are several limitations
that need to be considered.

Firstly, the computational overhead and associated expenses present
substantial challenges - every iteration requires multiple LLM calls
over multiple rounds of debate, crossover and mutation. The costs
scale linearly with population size, debate depth and number of
generations. This can quickly become prohibitively expensive in
production environments, particularly in complex agent systems
requiring frequent optimization and powerful models.

Secondly, the reliance on LLM-generated feedback through debates,
while scalable and autonomous, can introduce alignment issues, as
models develop their own implicit evaluation criteria without any
human intervention. The lack of feedback alignment can lead to
optimization towards criteria that may not align with real-world
business objectives, which can cause drift from desired performance
characteristics; however, this is a known trade-off in using AI feedback to optimize prompts and models [42].

Lastly, DEEVO lacks robust stopping criteria for practical deployment, as it can run indefinitely with marginal improvements to
the prompts. This makes it difficult to determine when optimized
prompts are "good enough" for production systems.

**6** **Conclusion**

In conclusion, we show how DEEVO addresses many of the limitations of existing prompt optimization approaches - specifically,
the ability to maintain the integrity and consistency of prompts
while allowing for meaningful exploration; the means to operate
without requiring ground truth labeled data or predefined fitness
functions; and a mechanism to track and maintain performance in
a self-supervised fashion.


Our evaluations demonstrate effectiveness across both controlled
benchmark datasets as well as real-world datasets, indicating robust
generalization capabilities across multiple domains. This optimization paradigm also opens new possibilities for prompt engineering
in domains where labeled data is scarce, expensive, or impractical
to obtain. As computational costs continue to decline and LLM
capabilities advance, we anticipate that DEEVO will increasingly
become the standard for developing high-performance prompts
across diverse applications.

While we acknowledge that our approach entails substantial computational costs, these expenses are insignificant when compared to
the investment required to develop and maintain specialized human
prompt engineering expertise. The iterative trial-and-error process
through which human engineers develop effective prompts often
takes time, whereas our automated system can achieve comparable
results far more rapidly, representing significant cost amortization
for organizations deploying LLM-based systems at scale.

**7** **Future Work**

Future work in prompt optimization presents several exciting frontiers, particularly in automated agent creation and holistic multiagent system optimization.

Recently, both evolutionary algorithms and multi-agent debate
have been used to automatically generate agentic teams [ 57 ] and
workflows [ 46 ]. We envision extrapolating our approach toward
fully automated agent creation, where these optimization systems
can dynamically determine the optimal number, types, and specializations of agents required for a given task, by adding, removing,
or merging agents based on performance metrics.

Additionally, methods for joint optimization of both multi-agent orchestration and sub-agent prompts, such as GPTSwarm [ 66 ], represent a critical advancement in which systems would simultaneously
evolve communication protocols, task delegation mechanisms, and
internal agent prompts. Such joint optimization would likely require
hierarchical evolutionary algorithms or multi-objective reinforcement learning approaches that balance agent-level and system-level
performance.

We anticipate that further research might also explore transfer
learning between tasks, allowing optimized agent configurations to
bootstrap performance on novel but related domains, thereby consolidating computational costs across multiple applications. These
advancements would move the field toward self-configuring multiagent systems that minimize human intervention while maximizing
performance across diverse tasks.

**References**

[1] Hisham A. Alyahya, Haidar Khan, Yazeed Alnumay, M Saiful Bari, and B√ºlent
Yener. 2025. ZeroSumEval: An Extensible Framework For Scaling LLM Evaluation
[with Inter-Model Competition. arXiv:2503.10673 [cs.CL] https://arxiv.org/abs/](https://arxiv.org/abs/2503.10673)
[2503.10673](https://arxiv.org/abs/2503.10673)

[2] Siavash Ameli, Siyuan Zhuang, Ion Stoica, and Michael W. Mahoney. 2024. A Sta[tistical Framework for Ranking LLM-Based Chatbots. arXiv:2412.18407 [stat.ML]](https://arxiv.org/abs/2412.18407)
[https://arxiv.org/abs/2412.18407](https://arxiv.org/abs/2412.18407)

[3] David Balduzzi, Karl Tuyls, Julien Perolat, and Thore Graepel. 2018. Re-evaluating
evaluation. _Advances in Neural Information Processing Systems_ 31 (2018).

Prompt Optimization KDD 2025, Toronto, Canada,
Tournament of Prompts: Evolving LLM Instructions Through Structured Debates and Elo Ratings



[4] Meriem Boubdir, Edward Kim, Beyza Ermis, Sara Hooker, and Marzieh Fadaee.
2023. Elo Uncovered: Robustness and Best Practices in Language Model Evalua[tion. arXiv:2311.17295 [cs.CL] https://arxiv.org/abs/2311.17295](https://arxiv.org/abs/2311.17295)

[5] Ralph Allan Bradley and Milton E Terry. 1952. Rank analysis of incomplete block
designs: I. The method of paired comparisons. _Biometrika_ 39, 3/4 (1952), 324‚Äì345.

[6] Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang
Zhang, Jie Fu, and Zhiyuan Liu. 2023. Chateval: Towards better llm-based evaluators through multi-agent debate. _arXiv preprint arXiv:2308.07201_ (2023).

[7] Derek Chen, Howard Chen, Yi Yang, Alex Lin, and Zhou Yu. 2021. Action-Based
Conversations Dataset: A Corpus for Building More In-Depth Task-Oriented
Dialogue Systems. In _Proceedings of the 2021 Conference of the North American_
_Chapter of the Association for Computational Linguistics: Human Language Tech-_
_nologies, NAACL-HLT 2021_ . Association for Computational Linguistics, Online,
[3002‚Äì3017. https://www.aclweb.org/anthology/2021.naacl-main.239](https://www.aclweb.org/anthology/2021.naacl-main.239)

[8] Lichang Chen, Jiuhai Chen, Tom Goldstein, Heng Huang, and Tianyi Zhou. 2023.
Instructzero: Efficient instruction optimization for black-box large language
models. _arXiv preprint arXiv:2306.03082_ (2023).

[9] Jiale Cheng, Xiao Liu, Kehan Zheng, Pei Ke, Hongning Wang, Yuxiao Dong, Jie
Tang, and Minlie Huang. 2023. Black-box prompt optimization: Aligning large
language models without model training. _arXiv preprint arXiv:2311.04155_ (2023).

[10] Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos,
Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph E.
Gonzalez, and Ion Stoica. 2024. Chatbot Arena: An Open Platform for Evaluating
[LLMs by Human Preference. arXiv:2403.04132 [cs.AI] https://arxiv.org/abs/2403.](https://arxiv.org/abs/2403.04132)
[04132](https://arxiv.org/abs/2403.04132)

[11] Mingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan Wang, Han Guo, Tianmin
Shu, Meng Song, Eric P Xing, and Zhiting Hu. 2022. RLPrompt: Optimizing
Discrete Text Prompts with Reinforcement Learning. In _EMNLP_ .

[12] Yihong Dong, Kangcheng Luo, Xue Jiang, Zhi Jin, and Ge Li. 2023. Pace: Improving prompt with actor-critic editing for large language model. _arXiv preprint_
_arXiv:2308.10088_ (2023).

[13] Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, and Igor Mordatch.
2023. Improving factuality and reasoning in language models through multiagent
debate. In _Forty-first International Conference on Machine Learning_ .

[14] Arpad E Elo and Sam Sloan. 1978. The rating of chessplayers: Past and present.
_(No Title)_ (1978).

[15] Andrew Estornell, Jean-Fran√ßois Ton, Yuanshun Yao, and Yang Liu. 2025. ACCcollab: An actor-critic approach to multi-agent LLM collaboration. In _The Thir-_
_teenth International Conference on Learning Representations_ .

[16] Chrisantha Fernando, Dylan Banarse, Henryk Michalewski, Simon Osindero, and
Tim Rockt√§schel. 2023. Promptbreeder: Self-referential self-improvement via
prompt evolution. _arXiv preprint arXiv:2309.16797_ (2023).

[17] Jiaxin Guo, Zewen Chi, Li Dong, Qingxiu Dong, Xun Wu, Shaohan Huang, and
Furu Wei. 2025. Reward Reasoning Model. _arXiv preprint arXiv:2505.14674_ (2025).

[18] Qingyan Guo, Rui Wang, Junliang Guo, Bei Li, Kaitao Song, Xu Tan, Guoqing Liu,
Jiang Bian, and Yujiu Yang. 2025. EvoPrompt: Connecting LLMs with Evolutionary
[Algorithms Yields Powerful Prompt Optimizers. arXiv:2309.08532 [cs.CL] https:](https://arxiv.org/abs/2309.08532)
[//arxiv.org/abs/2309.08532](https://arxiv.org/abs/2309.08532)

[19] Inman Harvey. 2009. The microbial genetic algorithm. In _European conference on_
_artificial life_ . Springer, 126‚Äì133.

[20] Rishi Hazra, Alkis Sygkounas, Andreas Persson, Amy Loutfi, and Pedro Zuidberg Dos Martires. 2024. REvolve: Reward Evolution with Large Language Models
using Human Feedback. _arXiv preprint arXiv:2406.01309_ (2024).

[21] Eshaan He et al . 2025. PromptWizard: Feedback-Driven Self-Evolving Prompt
Optimization. _Microsoft Research_ (2025).

[22] Yifan Hou et al . 2023. PromptBoosting: Boosting Prompt Optimization via SelfPlay. _arXiv preprint arXiv:2305.03495_ (2023).

[23] Wenyang Hu, Yao Shu, Zongmin Yu, Zhaoxuan Wu, Xiaoqiang Lin, Zhongxiang
Dai, See-Kiong Ng, and Bryan Kian Hsiang Low. 2024. Localized zeroth-order
prompt optimization. _Advances in Neural Information Processing Systems_ 37
(2024), 86309‚Äì86345.

[24] Abhinav Jain, Swarat Chaudhuri, Thomas Reps, and Chris Jermaine. 2024. Prompt
tuning strikes back: Customizing foundation models with low-rank prompt
adaptation. _arXiv preprint arXiv:2405.15282_ (2024).

[25] Yucheng Jiang, Yijia Shao, Dekun Ma, Sina J Semnani, and Monica S Lam. 2024.
Into the unknown unknowns: Engaged human learning through participation in
language model agent conversations. _arXiv preprint arXiv:2408.15232_ (2024).

[26] Qi Ju, Falin Hei, Zhemei Fang, and Yunfeng Luo. 2024. ELO-Rated Sequence
Rewards: Advancing Reinforcement Learning Models. In _2024 IEEE 13th Data_
_Driven Control and Learning Systems Conference (DDCLS)_ . IEEE, 2062‚Äì2069.

[27] Akbir Khan, John Hughes, Dan Valentine, Laura Ruis, Kshitij Sachan, Ansh
Radhakrishnan, Edward Grefenstette, Samuel R Bowman, Tim Rockt√§schel, and
Ethan Perez. 2024. Debating with more persuasive llms leads to more truthful
answers. _arXiv preprint arXiv:2402.06782_ (2024).

[28] Omar Khattab, Arnav Singhvi, Paridhi Maheshwari, Zhiyuan Zhang, Keshav
Santhanam, Saiful Haq, Ashutosh Sharma, Thomas T Joshi, Hanna Moazam,
Heather Miller, et al . 2024. Dspy: Compiling declarative language model calls
into state-of-the-art pipelines. In _The Twelfth International Conference on Learning_


_Representations_ .

[29] Alex Kim, Keonwoo Kim, and Sangwon Yoon. 2024. DEBATE: Devil‚Äôs AdvocateBased Assessment and Text Evaluation. _arXiv preprint arXiv:2405.09935_ (2024).

[30] Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The power of scale for
parameter-efficient prompt tuning. _arXiv preprint arXiv:2104.08691_ (2021).

[31] Chengzhengxu Li, Xiaoming Liu, Zhaohan Zhang, Yichen Wang, Chen Liu,
Yu Lan, and Chao Shen. 2024. Concentrate Attention: Towards DomainGeneralizable Prompt Optimization for Language Models. In _Advances in Neu-_
_ral Information Processing Systems_, A. Globerson, L. Mackey, D. Belgrave,
A. Fan, U. Paquet, J. Tomczak, and C. Zhang (Eds.), Vol. 37. Curran Associates,
Inc., 3391‚Äì3420. [https://proceedings.neurips.cc/paper_files/paper/2024/file/](https://proceedings.neurips.cc/paper_files/paper/2024/file/061d5d1b7d97117764f205d4e038f9eb-Paper-Conference.pdf)
[061d5d1b7d97117764f205d4e038f9eb-Paper-Conference.pdf](https://proceedings.neurips.cc/paper_files/paper/2024/file/061d5d1b7d97117764f205d4e038f9eb-Paper-Conference.pdf)

[32] Yan Li et al . 2024. Debatrix: Multi-agent LLM Debate for Scalable Evaluation.
_arXiv preprint arXiv:2402.13543_ (2024).

[33] Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang,
Yujiu Yang, Shuming Shi, and Zhaopeng Tu. 2023. Encouraging divergent
thinking in large language models through multi-agent debate. _arXiv preprint_
_arXiv:2305.19118_ (2023).

[34] Bill Yuchen Lin, Yuntian Deng, Khyathi Chandu, Faeze Brahman, Abhilasha
Ravichander, Valentina Pyatkin, Nouha Dziri, Ronan Le Bras, and Yejin Choi.
2024. Wildbench: Benchmarking llms with challenging tasks from real users in
the wild. _arXiv preprint arXiv:2406.04770_ (2024).

[35] Julian Michael, Salsabila Mahdi, David Rein, Jackson Petty, Julien Dirani, Vishakh
Padmakumar, and Samuel R Bowman. 2023. Debate helps supervise unreliable
experts. _arXiv preprint arXiv:2311.08702_ (2023).

[36] Ethan Perez, Siddharth Karamcheti, Rob Fergus, Jason Weston, Douwe Kiela, and
Kyunghyun Cho. 2019. Finding generalizable evidence by learning to convince
q&a models. _arXiv preprint arXiv:1909.05863_ (2019).

[37] Archiki Prasad, Peter Hase, Xiang Zhou, and Mohit Bansal. 2022. GrIPS: Gradientfree, Edit-based Instruction Search for Prompting Large Language Models. _arXiv_
_preprint arXiv:2203.07281_ (2022).

[38] Reid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chenguang Zhu, and Michael Zeng.
2023. Automatic Prompt Optimization with "Gradient Descent" and Beam Search.
[arXiv:2305.03495 [cs.CL] https://arxiv.org/abs/2305.03495](https://arxiv.org/abs/2305.03495)

[39] Reid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chenguang Zhu, and Michael Zeng.
2023. Automatic prompt optimization with" gradient descent" and beam search.
_arXiv preprint arXiv:2305.03495_ (2023).

[40] Ansh Radhakrishnan. 2023. Anthropic fall 2023 debate progress update. In _AI_
_Alignment Forum_, Vol. 80. 82‚Äì84.

[41] Pejaver V Rao and Lawrence L Kupper. 1967. Ties in paired-comparison experiments: A generalization of the Bradley-Terry model. _J. Amer. Statist. Assoc._ 62,
317 (1967), 194‚Äì204.

[42] Archit Sharma, Sedrick Scott Keh, Eric Mitchell, Chelsea Finn, Kushal Arora,
and Thomas Kollar. 2024. A critical evaluation of ai feedback for aligning large
language models. _Advances in Neural Information Processing Systems_ 37 (2024),
29166‚Äì29190.

[43] Taylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric Wallace, and Sameer Singh.
2020. Autoprompt: Eliciting knowledge from language models with automatically
generated prompts. _arXiv preprint arXiv:2010.15980_ (2020).

[44] Ankita Sinha, Wendi Cui, Kamalika Das, and Jiaxin Zhang. 2024. Survival of the
Safest: Towards Secure Prompt Optimization through Interleaved Multi-Objective
Evolution. _arXiv preprint arXiv:2410.09652_ (2024).

[45] Rainer Storn and Kenneth Price. 1997. Differential evolution‚Äìa simple and
efficient heuristic for global optimization over continuous spaces. _Journal of_
_global optimization_ 11 (1997), 341‚Äì359.

[46] Jinwei Su, Yinghui Xia, Ronghua Shi, Jianhui Wang, Jianuo Huang, Yijin Wang,
Tianyu Shi, Yang Jingsong, and Lewei He. 2025. DebFlow: Automating Agent
Creation via Agent Debate. _arXiv preprint arXiv:2503.23781_ (2025).

[47] Vighnesh Subramaniam, Antonio Torralba, and Shuang Li. 2024. Debategpt:
Fine-tuning large language models with multi-agent debate supervision. (2024).

[48] H. Sun et al . 2024. Query-Dependent Prompt Evaluation and Optimization with
Offline Inverse Reinforcement Learning. _arXiv preprint arXiv:2309.06553_ (2024).

[49] Tianxiang Sun, Yunfan Shao, Hong Qian, Xuanjing Huang, and Xipeng Qiu. 2022.
Black-box tuning for language-model-as-a-service. In _International Conference_
_on Machine Learning_ . PMLR, 20841‚Äì20855.

[50] Mirac Suzgun, Nathan Scales, Nathanael Sch√§rli, Sebastian Gehrmann, Yi Tay,
Hyung Won Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou,
et al . 2022. Challenging big-bench tasks and whether chain-of-thought can solve
them. _arXiv preprint arXiv:2210.09261_ (2022).

[51] Shange Tang, Yuanhao Wang, and Chi Jin. 2025. Is Elo Rating Reliable? A Study
Under Model Misspecification. _arXiv preprint arXiv:2502.10985_ (2025).

[52] Rose E. Wang, Qingyang Zhang, Carly Robinson, Susanna Loeb, and Dorottya
Demszky. 2024. Bridging the Novice-Expert Gap via Models of Decision-Making:
A Case Study on Remediating Math Mistakes. In _Proceedings of the 2024 Conference_
_of the North American Chapter of the Association for Computational Linguistics_ .
Association for Computational Linguistics.

[53] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi,
Quoc V Le, Denny Zhou, et al . 2022. Chain-of-thought prompting elicits reasoning

Prompt Optimization KDD 2025, Toronto, Canada,
Nair and Banerjee et al.


in large language models. _Advances in neural information processing systems_ 35
(2022), 24824‚Äì24837.

[54] Jinyu Xiang, Jiayi Zhang, Zhaoyang Yu, Fengwei Teng, Jinhao Tu, Xinbing Liang,
Sirui Hong, Chenglin Wu, and Yuyu Luo. 2025. Self-Supervised Prompt Optimization. _arXiv preprint arXiv:2502.06855_ (2025).

[55] Cilin Yan, Jingyun Wang, Lin Zhang, Ruihui Zhao, Xiaopu Wu, Kai Xiong, Qingsong Liu, Guoliang Kang, and Yangyang Kang. 2024. Efficient and Accurate
Prompt Optimization: the Benefit of Memory in Exemplar-Guided Reflection.
_arXiv preprint arXiv:2411.07446_ (2024).

[56] Jiayi Ye, Yanbo Wang, Yue Huang, Dongping Chen, Qihui Zhang, Nuno Moniz,
Tian Gao, Werner Geyer, Chao Huang, Pin-Yu Chen, et al . 2024. Justice or
prejudice? quantifying biases in llm-as-a-judge. _arXiv preprint arXiv:2410.02736_
(2024).

[57] Siyu Yuan, Kaitao Song, Jiangjie Chen, Xu Tan, Dongsheng Li, and Deqing Yang.
2024. Evoagent: Towards automatic multi-agent generation via evolutionary
algorithms. _arXiv preprint arXiv:2406.14228_ (2024).

[58] Mert Yuksekgonul, Federico Bianchi, Joseph Boen, Sheng Liu, Zhi Huang, Carlos
Guestrin, and James Zou. 2024. TextGrad: Automatic "Differentiation" via Text.
[arXiv:2406.07496 [cs.CL] https://arxiv.org/abs/2406.07496](https://arxiv.org/abs/2406.07496)

[59] Jiayi Zhang et al . 2024. PREFER: Prompt Optimization with Feedback and Refinement. _arXiv preprint arXiv:2406.07496_ (2024).

[60] Ningyu Zhang, Luoqiu Li, Xiang Chen, Shumin Deng, Zhen Bi, Chuanqi Tan,
Fei Huang, and Huajun Chen. 2021. Differentiable prompt makes pre-trained
language models better few-shot learners. _arXiv preprint arXiv:2108.13161_ (2021).

[61] Tianjun Zhang, Xuezhi Wang, Denny Zhou, Dale Schuurmans, and Joseph E Gonzalez. 2023. TEMPERA: Test-Time Prompt Editing via Reinforcement Learning.
In _ICLR_ .

[62] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu,
Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al . 2023. Judging
llm-as-a-judge with mt-bench and chatbot arena. _Advances in Neural Information_
_Processing Systems_ 36 (2023), 46595‚Äì46623.

[63] Han Zhou, Xingchen Wan, Yinhong Liu, Nigel Collier, Ivan Vuliƒá, and Anna
Korhonen. 2024. Fairer preferences elicit improved human-aligned large language
model judgments. _arXiv preprint arXiv:2406.11370_ (2024).

[64] Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis,
Harris Chan, and Jimmy Ba. 2022. Large language models are human-level prompt
engineers. In _The Eleventh International Conference on Learning Representations_ .

[65] Ziyi Zhu et al . 2024. Bayesian Dynamic Prompt Learning. _arXiv preprint_
_arXiv:2402.11344_ (2024).

[66] Mingchen Zhuge, Wenyi Wang, Louis Kirsch, Francesco Faccio, Dmitrii
Khizbullin, and J√ºrgen Schmidhuber. 2024. Gptswarm: Language agents as
optimizable graphs. In _Forty-first International Conference on Machine Learning_ .


**A** **Appendix**

**A.1** **Debate Defender System Prompt**

6

The example debater prompt defending output B. The other debater
agent uses the same prompt but instead defends output A.

**A.2** **Debate Strategy**

3

7

11

15

17

21

25

29

31

34

Prompt Optimization KDD 2025, Toronto, Canada,
Tournament of Prompts: Evolving LLM Instructions Through Structured Debates and Elo Ratings


**A.3** **Debate Transcript Example - ABCD**

2

6

10

13

15


19

23

26

Prompt Optimization KDD 2025, Toronto, Canada,
Nair and Banerjee et al.


28

32

36


39

**A.4** **Optimized Prompt Example - ABCD**

2

4

10

Prompt Optimization KDD 2025, Toronto, Canada,
Tournament of Prompts: Evolving LLM Instructions Through Structured Debates and Elo Ratings


18

20

24

28

32

36

40

44

48


**A.5** **Optimized Prompt Example - MT-Bench**
**(Writing)**

1

3

10

17

24

31

38

40

**A.6** **Optimized Prompt Example - MT-Bench**
**(Roleplay)**

Prompt Optimization KDD 2025, Toronto, Canada,
Nair and Banerjee et al.


2

9

16

23

30

**A.7** **Optimized Prompt Example - MT-Bench**
**(Humanities)**

2

9


16

23

30

37

44

Prompt Optimization KDD 2025, Toronto, Canada,
Tournament of Prompts: Evolving LLM Instructions Through Structured Debates and Elo Ratings


**A.8** **Intelligent Crossover Prompt**

2

5

8

11

15

**A.9** **Intelligent Mutation Prompt**

2

5

12


18

